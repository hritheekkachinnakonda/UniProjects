{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training for n1=1, n2=1\n",
      "Early stopping at epoch 25\n",
      "Run 1: Best Training Loss: 0.6893040259294959, Best Validation Loss: 0.6896222499923816, Misclassification Error (Test): 0.39636363636363636\n",
      "Early stopping at epoch 22\n",
      "Run 2: Best Training Loss: 0.6894060659868593, Best Validation Loss: 0.6898096669860219, Misclassification Error (Test): 0.39636363636363636\n",
      "Early stopping at epoch 104\n",
      "Run 3: Best Training Loss: 0.6892693508179488, Best Validation Loss: 0.6895949897986173, Misclassification Error (Test): 0.39636363636363636\n",
      "\n",
      "Training for n1=1, n2=2\n",
      "Early stopping at epoch 92\n",
      "Run 1: Best Training Loss: 0.6895052274381627, Best Validation Loss: 0.6897650707111483, Misclassification Error (Test): 0.39636363636363636\n",
      "Early stopping at epoch 39\n",
      "Run 2: Best Training Loss: 0.6892657468069319, Best Validation Loss: 0.6895912326468786, Misclassification Error (Test): 0.39636363636363636\n",
      "Early stopping at epoch 33\n",
      "Run 3: Best Training Loss: 0.6892979314721265, Best Validation Loss: 0.6896128558254614, Misclassification Error (Test): 0.39636363636363636\n",
      "\n",
      "Training for n1=1, n2=3\n",
      "Early stopping at epoch 46\n",
      "Run 1: Best Training Loss: 0.6892676167262991, Best Validation Loss: 0.689594269859601, Misclassification Error (Test): 0.39636363636363636\n",
      "Early stopping at epoch 35\n",
      "Run 2: Best Training Loss: 0.6893975733791827, Best Validation Loss: 0.6897675346689092, Misclassification Error (Test): 0.39636363636363636\n",
      "Early stopping at epoch 21\n",
      "Run 3: Best Training Loss: 0.6895462125570073, Best Validation Loss: 0.6898858118930408, Misclassification Error (Test): 0.39636363636363636\n",
      "\n",
      "Training for n1=1, n2=4\n",
      "Early stopping at epoch 21\n",
      "Run 1: Best Training Loss: 0.6902836788078592, Best Validation Loss: 0.6906201210910968, Misclassification Error (Test): 0.36363636363636365\n",
      "Early stopping at epoch 26\n",
      "Run 2: Best Training Loss: 0.6894710206024253, Best Validation Loss: 0.6898669444197678, Misclassification Error (Test): 0.39636363636363636\n",
      "Early stopping at epoch 41\n",
      "Run 3: Best Training Loss: 0.6892709418410186, Best Validation Loss: 0.6895966227084354, Misclassification Error (Test): 0.39636363636363636\n",
      "\n",
      "Training for n1=1, n2=5\n",
      "Early stopping at epoch 23\n",
      "Run 1: Best Training Loss: 0.6892661179080287, Best Validation Loss: 0.689599870515039, Misclassification Error (Test): 0.39636363636363636\n",
      "Early stopping at epoch 42\n",
      "Run 2: Best Training Loss: 0.6893157797538373, Best Validation Loss: 0.6896569001006487, Misclassification Error (Test): 0.39636363636363636\n",
      "Early stopping at epoch 28\n",
      "Run 3: Best Training Loss: 0.6893791507328658, Best Validation Loss: 0.6897440275180861, Misclassification Error (Test): 0.39636363636363636\n",
      "\n",
      "Training for n1=1, n2=6\n",
      "Early stopping at epoch 19\n",
      "Run 1: Best Training Loss: 0.6896877602173714, Best Validation Loss: 0.6900546415371714, Misclassification Error (Test): 0.39636363636363636\n",
      "Early stopping at epoch 21\n",
      "Run 2: Best Training Loss: 0.6906259280359852, Best Validation Loss: 0.6909259038115211, Misclassification Error (Test): 0.29454545454545455\n",
      "Early stopping at epoch 33\n",
      "Run 3: Best Training Loss: 0.6892667305762742, Best Validation Loss: 0.6895913158828898, Misclassification Error (Test): 0.39636363636363636\n",
      "\n",
      "Training for n1=1, n2=7\n",
      "Early stopping at epoch 22\n",
      "Run 1: Best Training Loss: 0.6898136155086388, Best Validation Loss: 0.6900980363198238, Misclassification Error (Test): 0.39636363636363636\n",
      "Early stopping at epoch 20\n",
      "Run 2: Best Training Loss: 0.6892805012521855, Best Validation Loss: 0.6895970856281006, Misclassification Error (Test): 0.39636363636363636\n",
      "Early stopping at epoch 22\n",
      "Run 3: Best Training Loss: 0.689276291083218, Best Validation Loss: 0.6896090288492602, Misclassification Error (Test): 0.39636363636363636\n",
      "\n",
      "Training for n1=2, n2=1\n",
      "Early stopping at epoch 27\n",
      "Run 1: Best Training Loss: 0.689314706499063, Best Validation Loss: 0.6896422011558987, Misclassification Error (Test): 0.39636363636363636\n",
      "Early stopping at epoch 57\n",
      "Run 2: Best Training Loss: 0.6902036497178341, Best Validation Loss: 0.6907057439250621, Misclassification Error (Test): 0.39636363636363636\n",
      "Early stopping at epoch 29\n",
      "Run 3: Best Training Loss: 0.6901374230390764, Best Validation Loss: 0.690634777920301, Misclassification Error (Test): 0.39636363636363636\n",
      "\n",
      "Training for n1=2, n2=2\n",
      "Early stopping at epoch 45\n",
      "Run 1: Best Training Loss: 0.689593363978719, Best Validation Loss: 0.6898608554226331, Misclassification Error (Test): 0.39636363636363636\n",
      "Early stopping at epoch 63\n",
      "Run 2: Best Training Loss: 0.6898371844182244, Best Validation Loss: 0.690045178516662, Misclassification Error (Test): 0.39636363636363636\n",
      "Early stopping at epoch 33\n",
      "Run 3: Best Training Loss: 0.6899364615759995, Best Validation Loss: 0.6902278846367188, Misclassification Error (Test): 0.39636363636363636\n",
      "\n",
      "Training for n1=2, n2=3\n",
      "Early stopping at epoch 36\n",
      "Run 1: Best Training Loss: 0.6906030434042676, Best Validation Loss: 0.6911465827109244, Misclassification Error (Test): 0.39636363636363636\n",
      "Early stopping at epoch 36\n",
      "Run 2: Best Training Loss: 0.6893238295971512, Best Validation Loss: 0.6896284482155216, Misclassification Error (Test): 0.39636363636363636\n",
      "Early stopping at epoch 36\n",
      "Run 3: Best Training Loss: 0.6893344786215708, Best Validation Loss: 0.6896510358581169, Misclassification Error (Test): 0.39636363636363636\n",
      "\n",
      "Training for n1=2, n2=4\n",
      "Early stopping at epoch 48\n",
      "Run 1: Best Training Loss: 0.6893463465164673, Best Validation Loss: 0.6896854626074673, Misclassification Error (Test): 0.39636363636363636\n",
      "Early stopping at epoch 25\n",
      "Run 2: Best Training Loss: 0.6892768862316455, Best Validation Loss: 0.6895967014271499, Misclassification Error (Test): 0.39636363636363636\n",
      "Early stopping at epoch 54\n",
      "Run 3: Best Training Loss: 0.6892810074933078, Best Validation Loss: 0.6896182912373617, Misclassification Error (Test): 0.39636363636363636\n",
      "\n",
      "Training for n1=2, n2=5\n",
      "Early stopping at epoch 24\n",
      "Run 1: Best Training Loss: 0.6900037622791506, Best Validation Loss: 0.6901955748888049, Misclassification Error (Test): 0.39636363636363636\n",
      "Early stopping at epoch 31\n",
      "Run 2: Best Training Loss: 0.6893439132243533, Best Validation Loss: 0.6896361121941875, Misclassification Error (Test): 0.39636363636363636\n",
      "Early stopping at epoch 22\n",
      "Run 3: Best Training Loss: 0.690415994962556, Best Validation Loss: 0.6905550961239724, Misclassification Error (Test): 0.3890909090909091\n",
      "\n",
      "Training for n1=2, n2=6\n",
      "Early stopping at epoch 24\n",
      "Run 1: Best Training Loss: 0.6900353049255894, Best Validation Loss: 0.6904189800029116, Misclassification Error (Test): 0.39636363636363636\n",
      "Early stopping at epoch 25\n",
      "Run 2: Best Training Loss: 0.689594236302759, Best Validation Loss: 0.6898831505510647, Misclassification Error (Test): 0.39636363636363636\n",
      "Early stopping at epoch 28\n",
      "Run 3: Best Training Loss: 0.6896789471598109, Best Validation Loss: 0.690008885242359, Misclassification Error (Test): 0.3927272727272727\n",
      "\n",
      "Training for n1=3, n2=1\n",
      "Early stopping at epoch 37\n",
      "Run 1: Best Training Loss: 0.6892947325495165, Best Validation Loss: 0.6896201152434981, Misclassification Error (Test): 0.39636363636363636\n",
      "Early stopping at epoch 91\n",
      "Run 2: Best Training Loss: 0.6892920643393367, Best Validation Loss: 0.6896129189982793, Misclassification Error (Test): 0.39636363636363636\n",
      "Early stopping at epoch 39\n",
      "Run 3: Best Training Loss: 0.6892785478104582, Best Validation Loss: 0.689609640784775, Misclassification Error (Test): 0.39636363636363636\n",
      "\n",
      "Training for n1=3, n2=2\n",
      "Early stopping at epoch 50\n",
      "Run 1: Best Training Loss: 0.6939980089861331, Best Validation Loss: 0.6939672476813932, Misclassification Error (Test): 0.6036363636363636\n",
      "Early stopping at epoch 27\n",
      "Run 2: Best Training Loss: 0.6892792141308379, Best Validation Loss: 0.6896026837780352, Misclassification Error (Test): 0.39636363636363636\n",
      "Early stopping at epoch 41\n",
      "Run 3: Best Training Loss: 0.6902197241146986, Best Validation Loss: 0.6904766649862124, Misclassification Error (Test): 0.38545454545454544\n",
      "\n",
      "Training for n1=3, n2=3\n",
      "Early stopping at epoch 29\n",
      "Run 1: Best Training Loss: 0.6893127889816524, Best Validation Loss: 0.6896370145448996, Misclassification Error (Test): 0.39636363636363636\n",
      "Early stopping at epoch 25\n",
      "Run 2: Best Training Loss: 0.6928374809432349, Best Validation Loss: 0.6930985008995841, Misclassification Error (Test): 0.24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 34\n",
      "Run 3: Best Training Loss: 0.6894478252992187, Best Validation Loss: 0.6897212591614569, Misclassification Error (Test): 0.39636363636363636\n",
      "\n",
      "Training for n1=3, n2=4\n",
      "Early stopping at epoch 34\n",
      "Run 1: Best Training Loss: 0.6894371270773055, Best Validation Loss: 0.6898213160233936, Misclassification Error (Test): 0.39636363636363636\n",
      "Early stopping at epoch 29\n",
      "Run 2: Best Training Loss: 0.6894117342557359, Best Validation Loss: 0.6897162656290585, Misclassification Error (Test): 0.39636363636363636\n",
      "Early stopping at epoch 27\n",
      "Run 3: Best Training Loss: 0.6894453449039676, Best Validation Loss: 0.6898149762874128, Misclassification Error (Test): 0.39636363636363636\n",
      "\n",
      "Training for n1=3, n2=5\n",
      "Early stopping at epoch 27\n",
      "Run 1: Best Training Loss: 0.689892981382161, Best Validation Loss: 0.690222197536311, Misclassification Error (Test): 0.39636363636363636\n",
      "Early stopping at epoch 25\n",
      "Run 2: Best Training Loss: 0.6900356303558334, Best Validation Loss: 0.6904873434377046, Misclassification Error (Test): 0.39636363636363636\n",
      "Early stopping at epoch 15\n",
      "Run 3: Best Training Loss: 0.6893972921129462, Best Validation Loss: 0.6896799471857828, Misclassification Error (Test): 0.39636363636363636\n",
      "\n",
      "Training for n1=4, n2=1\n",
      "Early stopping at epoch 46\n",
      "Run 1: Best Training Loss: 0.6990768473862187, Best Validation Loss: 0.698863487693176, Misclassification Error (Test): 0.6036363636363636\n",
      "Early stopping at epoch 58\n",
      "Run 2: Best Training Loss: 0.6982249882771794, Best Validation Loss: 0.6980572710750541, Misclassification Error (Test): 0.6036363636363636\n",
      "Run 3: Best Training Loss: 0.6911365199320524, Best Validation Loss: 0.6912411979657842, Misclassification Error (Test): 0.39636363636363636\n",
      "\n",
      "Training for n1=4, n2=2\n",
      "Early stopping at epoch 53\n",
      "Run 1: Best Training Loss: 0.6893817588189146, Best Validation Loss: 0.689699126238837, Misclassification Error (Test): 0.39636363636363636\n",
      "Early stopping at epoch 30\n",
      "Run 2: Best Training Loss: 0.6924631465988351, Best Validation Loss: 0.692491319806553, Misclassification Error (Test): 0.14545454545454545\n",
      "Early stopping at epoch 21\n",
      "Run 3: Best Training Loss: 0.6894868875218236, Best Validation Loss: 0.6897950008749417, Misclassification Error (Test): 0.39636363636363636\n",
      "\n",
      "Training for n1=4, n2=3\n",
      "Early stopping at epoch 31\n",
      "Run 1: Best Training Loss: 0.6894770791323582, Best Validation Loss: 0.6898866511169519, Misclassification Error (Test): 0.39636363636363636\n",
      "Early stopping at epoch 26\n",
      "Run 2: Best Training Loss: 0.6898708351982283, Best Validation Loss: 0.6902501726805467, Misclassification Error (Test): 0.39636363636363636\n",
      "Early stopping at epoch 23\n",
      "Run 3: Best Training Loss: 0.6896088597203338, Best Validation Loss: 0.6899449251654721, Misclassification Error (Test): 0.39636363636363636\n",
      "\n",
      "Training for n1=4, n2=4\n",
      "Early stopping at epoch 22\n",
      "Run 1: Best Training Loss: 0.6894343261479092, Best Validation Loss: 0.6897828331852217, Misclassification Error (Test): 0.39636363636363636\n",
      "Early stopping at epoch 33\n",
      "Run 2: Best Training Loss: 0.6899925044125815, Best Validation Loss: 0.6902192593788419, Misclassification Error (Test): 0.39636363636363636\n",
      "Early stopping at epoch 27\n",
      "Run 3: Best Training Loss: 0.6897904997037563, Best Validation Loss: 0.6900489216294661, Misclassification Error (Test): 0.39636363636363636\n",
      "\n",
      "Training for n1=5, n2=1\n",
      "Early stopping at epoch 39\n",
      "Run 1: Best Training Loss: 0.6904512033756528, Best Validation Loss: 0.6909735973419918, Misclassification Error (Test): 0.39636363636363636\n",
      "Early stopping at epoch 33\n",
      "Run 2: Best Training Loss: 0.6893898733076693, Best Validation Loss: 0.689700925773135, Misclassification Error (Test): 0.39636363636363636\n",
      "Early stopping at epoch 22\n",
      "Run 3: Best Training Loss: 0.6893225688474751, Best Validation Loss: 0.6896484697769045, Misclassification Error (Test): 0.39636363636363636\n",
      "\n",
      "Training for n1=5, n2=2\n",
      "Early stopping at epoch 36\n",
      "Run 1: Best Training Loss: 0.689287807610667, Best Validation Loss: 0.6896185009740168, Misclassification Error (Test): 0.39636363636363636\n",
      "Early stopping at epoch 46\n",
      "Run 2: Best Training Loss: 0.6892815992064546, Best Validation Loss: 0.6896087589096463, Misclassification Error (Test): 0.39636363636363636\n",
      "Early stopping at epoch 29\n",
      "Run 3: Best Training Loss: 0.6895239186195243, Best Validation Loss: 0.6898455444206786, Misclassification Error (Test): 0.39636363636363636\n",
      "\n",
      "Training for n1=5, n2=3\n",
      "Early stopping at epoch 22\n",
      "Run 1: Best Training Loss: 0.6894814270699812, Best Validation Loss: 0.6898090807689936, Misclassification Error (Test): 0.39636363636363636\n",
      "Early stopping at epoch 26\n",
      "Run 2: Best Training Loss: 0.689734213143535, Best Validation Loss: 0.6900242508431922, Misclassification Error (Test): 0.39636363636363636\n",
      "Early stopping at epoch 20\n",
      "Run 3: Best Training Loss: 0.6917015677545538, Best Validation Loss: 0.692292376050293, Misclassification Error (Test): 0.36\n",
      "\n",
      "Training for n1=6, n2=1\n",
      "Early stopping at epoch 15\n",
      "Run 1: Best Training Loss: 0.6898842354163853, Best Validation Loss: 0.6901341914694329, Misclassification Error (Test): 0.39636363636363636\n",
      "Early stopping at epoch 40\n",
      "Run 2: Best Training Loss: 0.7090088946174455, Best Validation Loss: 0.7084006478530558, Misclassification Error (Test): 0.6036363636363636\n",
      "Early stopping at epoch 40\n",
      "Run 3: Best Training Loss: 0.6893165109345466, Best Validation Loss: 0.6896391836019841, Misclassification Error (Test): 0.39636363636363636\n",
      "\n",
      "Training for n1=6, n2=2\n",
      "Early stopping at epoch 30\n",
      "Run 1: Best Training Loss: 0.6895795644009413, Best Validation Loss: 0.6898884857944947, Misclassification Error (Test): 0.39636363636363636\n",
      "Early stopping at epoch 25\n",
      "Run 2: Best Training Loss: 0.6894064795176716, Best Validation Loss: 0.6897035374412576, Misclassification Error (Test): 0.39636363636363636\n",
      "Early stopping at epoch 30\n",
      "Run 3: Best Training Loss: 0.6906367248220521, Best Validation Loss: 0.6908972902123329, Misclassification Error (Test): 0.4\n",
      "\n",
      "Training for n1=7, n2=1\n",
      "Early stopping at epoch 18\n",
      "Run 1: Best Training Loss: 0.6919385020163231, Best Validation Loss: 0.6925147831189434, Misclassification Error (Test): 0.39636363636363636\n",
      "Early stopping at epoch 21\n",
      "Run 2: Best Training Loss: 0.6893482435903153, Best Validation Loss: 0.6896750395290187, Misclassification Error (Test): 0.39636363636363636\n",
      "Early stopping at epoch 75\n",
      "Run 3: Best Training Loss: 0.6892818973731628, Best Validation Loss: 0.6896066063028643, Misclassification Error (Test): 0.39636363636363636\n",
      "\n",
      "Best Network Configuration: n1=1, n2=2\n",
      "Best Validation Loss: 0.6895912326468786\n",
      "Best Training Loss: 0.6892818973731628\n",
      "Best Misclassification Error (Test): 0.39636363636363636\n",
      "Early stopping at epoch 15\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Misclassification Error: 0.45592705167173253\n",
      "Validation Misclassification Error: 0.45785876993166286\n",
      "Test Misclassification Error: 0.39636363636363636\n",
      "Model weights: {'hidden_L1_weights': array([[0.61067514],\n",
      "       [0.48035671],\n",
      "       [0.02209547],\n",
      "       [0.22800403]]), 'hidden_L2_weights': array([[-0.38226279, -0.01171736]]), 'output_layer_weights': array([[ 0.33447926],\n",
      "       [-0.6200422 ]])}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "    return sigmoid(z) * (1 - sigmoid(z))\n",
    "\n",
    "def initialize_weights(input_dim, n1, n2, random_state):\n",
    "    hidden_L1_weights = 2 * np.random.rand(input_dim, n1) - 1\n",
    "    hidden_L2_weights = 2 * np.random.rand(n1, n2) - 1\n",
    "    output_layer_weights = 2 * np.random.rand(n2, 1) - 1\n",
    "    return hidden_L1_weights, hidden_L2_weights, output_layer_weights\n",
    "\n",
    "def forward_propagation(X, hidden_L1_weights, hidden_L2_weights, output_layer_weights):\n",
    "    hidden_L1_output = sigmoid(np.dot(X, hidden_L1_weights))\n",
    "    hidden_L2_output = sigmoid(np.dot(hidden_L1_output, hidden_L2_weights))\n",
    "    output = sigmoid(np.dot(hidden_L2_output, output_layer_weights))\n",
    "    return hidden_L1_output, hidden_L2_output, output\n",
    "\n",
    "def compute_loss(y, output):\n",
    "    loss = -np.mean(y * np.log(output) + (1 - y) * np.log(1 - output))\n",
    "    return loss\n",
    "\n",
    "def backpropagation(X_train, y_train, hidden_L1_weights, hidden_L2_weights, output_layer_weights, learning_rate):\n",
    "    hidden_L1_output, hidden_L2_output, output = forward_propagation(X_train, hidden_L1_weights, hidden_L2_weights, output_layer_weights)\n",
    "\n",
    "    output_error = y_train.reshape(-1, 1) - output\n",
    "    output_delta = output_error * sigmoid_derivative(output)\n",
    "\n",
    "    hidden_L2_error = output_delta.dot(output_layer_weights.T)\n",
    "    hidden_L2_delta = hidden_L2_error * sigmoid_derivative(hidden_L2_output)\n",
    "\n",
    "    hidden_L1_error = hidden_L2_delta.dot(hidden_L2_weights.T)\n",
    "    hidden_L1_delta = hidden_L1_error * sigmoid_derivative(hidden_L1_output)\n",
    "\n",
    "    output_layer_weights += hidden_L2_output.T.dot(output_delta) * learning_rate\n",
    "    hidden_L2_weights += hidden_L1_output.T.dot(hidden_L2_delta) * learning_rate\n",
    "    hidden_L1_weights += X_train.T.dot(hidden_L1_delta) * learning_rate\n",
    "\n",
    "    return hidden_L1_weights, hidden_L2_weights, output_layer_weights\n",
    "\n",
    "def train_neural_network(X_train, y_train, X_validation, y_validation, hidden_L1_weights, hidden_L2_weights, output_layer_weights, learning_rate, epochs, early_stopping_patience):\n",
    "    training_loss_values = []\n",
    "    validation_loss_values = []\n",
    "    \n",
    "    best_validation_loss = float('inf')\n",
    "    patience = 0\n",
    "    optimal_weights = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        hidden_L1_weights, hidden_L2_weights, output_layer_weights = backpropagation(X_train, y_train, hidden_L1_weights, hidden_L2_weights, output_layer_weights, learning_rate)\n",
    "\n",
    "        hidden_L1_output_val, hidden_L2_output_val, output_val = forward_propagation(X_validation, hidden_L1_weights, hidden_L2_weights, output_layer_weights)\n",
    "\n",
    "        training_loss = compute_loss(y_train, forward_propagation(X_train, hidden_L1_weights, hidden_L2_weights, output_layer_weights)[-1])\n",
    "        training_loss_values.append(training_loss)\n",
    "\n",
    "        validation_loss = compute_loss(y_validation, output_val)\n",
    "        validation_loss_values.append(validation_loss)\n",
    "\n",
    "        if validation_loss < best_validation_loss:\n",
    "            optimal_weights = {\n",
    "                'hidden_L1_weights': hidden_L1_weights.copy(),\n",
    "                'hidden_L2_weights': hidden_L2_weights.copy(),\n",
    "                'output_layer_weights': output_layer_weights.copy()\n",
    "            }\n",
    "            best_validation_loss = validation_loss\n",
    "            best_training_loss = training_loss\n",
    "            patience = 0\n",
    "        else:\n",
    "            patience += 1\n",
    "            if patience >= early_stopping_patience:\n",
    "                print(f'Early stopping at epoch {epoch}')\n",
    "                break\n",
    "\n",
    "    return optimal_weights, best_training_loss, best_validation_loss, training_loss_values, validation_loss_values\n",
    "\n",
    "def test_neural_network(X, y, optimal_weights):\n",
    "    hidden_L1_output, hidden_L2_output, output = forward_propagation(X, optimal_weights['hidden_L1_weights'], optimal_weights['hidden_L2_weights'], optimal_weights['output_layer_weights'])\n",
    "    test_predictions = (output > 0.5).astype(int)\n",
    "    misclassification_error = np.mean(test_predictions.flatten() != y)\n",
    "    return misclassification_error\n",
    "\n",
    "def main():\n",
    "    # Loading the dataset\n",
    "    data = pd.read_csv('data_banknote_authentication.txt', header=None, names=['Feature1', 'Feature2', 'Feature3', 'Feature4', 'Class'])\n",
    "    X = data.iloc[:, :4].values\n",
    "    y = data.iloc[:, 4].values\n",
    "\n",
    "    random_state = 2782  # Last 4 digits of student number\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    learning_rate = 0.005\n",
    "    epochs = 150\n",
    "    early_stopping_patience = 15\n",
    "    max_hidden_units = 8\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_state)\n",
    "    X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=0.4, random_state=random_state)\n",
    "\n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_validation = scaler.transform(X_validation)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    # Initialize optimization parameters\n",
    "    optimized_n1 = None\n",
    "    optimized_n2 = None\n",
    "    optimized_weights = None\n",
    "    optimized_validation_loss = float('inf')\n",
    "    optimized_misclassification_error = float('inf')\n",
    "\n",
    "    for n1 in range(1, max_hidden_units):\n",
    "        for n2 in range(1, max_hidden_units - n1 + 1):\n",
    "            print(f'\\nTraining for n1={n1}, n2={n2}')\n",
    "            optimal_weights_list = []\n",
    "            best_training_loss_list = []\n",
    "            best_validation_loss_list = []\n",
    "            misclassification_error_test_list = []\n",
    "\n",
    "            for run in range(3):  # Run at least three times with different initial weights\n",
    "                input_dim = X_train.shape[1]\n",
    "                hidden_L1_weights, hidden_L2_weights, output_layer_weights = initialize_weights(input_dim, n1, n2, random_state + run)\n",
    "\n",
    "                optimal_weights, best_training_loss, best_validation_loss, training_loss_values, validation_loss_values = train_neural_network(X_train, y_train, X_validation, y_validation, hidden_L1_weights, hidden_L2_weights, output_layer_weights, learning_rate, epochs, early_stopping_patience)\n",
    "                misclassification_error_test = test_neural_network(X_test, y_test, optimal_weights)\n",
    "\n",
    "                optimal_weights_list.append(optimal_weights)\n",
    "                best_training_loss_list.append(best_training_loss)\n",
    "                best_validation_loss_list.append(best_validation_loss)\n",
    "                misclassification_error_test_list.append(misclassification_error_test)\n",
    "\n",
    "                print(f'Run {run + 1}: Best Training Loss: {best_training_loss}, Best Validation Loss: {best_validation_loss}, Misclassification Error (Test): {misclassification_error_test}')\n",
    "\n",
    "            # Choose the best initialization for this network configuration\n",
    "            best_index = np.argmin(best_validation_loss_list)\n",
    "            if best_validation_loss_list[best_index] < optimized_validation_loss:\n",
    "                optimized_n1 = n1\n",
    "                optimized_n2 = n2\n",
    "                optimized_weights = optimal_weights_list[best_index]\n",
    "                optimized_validation_loss = best_validation_loss_list[best_index]\n",
    "                optimized_misclassification_error = misclassification_error_test_list[best_index]\n",
    "\n",
    "    # Print and plot results for the best network\n",
    "    print(f'\\nBest Network Configuration: n1={optimized_n1}, n2={optimized_n2}')\n",
    "    print(f'Best Validation Loss: {optimized_validation_loss}')\n",
    "    print(f'Best Training Loss: {best_training_loss_list[best_index]}')\n",
    "    print(f'Best Misclassification Error (Test): {optimized_misclassification_error}')\n",
    "\n",
    "    # Extract loss values during training and validation from your training loop\n",
    "    best_training_loss_values, best_validation_loss_values = train_neural_network(X_train, y_train, X_validation, y_validation, optimized_weights['hidden_L1_weights'], optimized_weights['hidden_L2_weights'], optimized_weights['output_layer_weights'], learning_rate, epochs, early_stopping_patience)[3:5]\n",
    "\n",
    "    # Plotting learning curves for the best network\n",
    "    plt.plot(best_training_loss_values, label='Training Loss')\n",
    "    plt.plot(best_validation_loss_values, label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Evaluate misclassification error for the final model\n",
    "    final_model_misclassification_error_train = test_neural_network(X_train, y_train, optimized_weights)\n",
    "    final_model_misclassification_error_validation = test_neural_network(X_validation, y_validation, optimized_weights)\n",
    "    final_model_misclassification_error_test = test_neural_network(X_test, y_test, optimized_weights)\n",
    "\n",
    "    print(f'\\nTraining Misclassification Error: {final_model_misclassification_error_train}')\n",
    "    print(f'Validation Misclassification Error: {final_model_misclassification_error_validation}')\n",
    "    print(f'Test Misclassification Error: {final_model_misclassification_error_test}')\n",
    "    print(f'Model weights: {optimized_weights}')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training for n1=1, n2=1\n",
      "Early stopping at epoch 26\n",
      "Run 1: Best Training Loss: 0.6893040259294959, Best Validation Loss: 0.6896222499923816, Misclassification Error (Test): 0.39636363636363636\n",
      "Early stopping at epoch 87\n",
      "Run 2: Best Training Loss: 0.6892988924091249, Best Validation Loss: 0.6896200069694273, Misclassification Error (Test): 0.39636363636363636\n",
      "Early stopping at epoch 53\n",
      "Run 3: Best Training Loss: 0.689284966219207, Best Validation Loss: 0.6896082749721109, Misclassification Error (Test): 0.39636363636363636\n",
      "\n",
      "Training for n1=1, n2=2\n",
      "Early stopping at epoch 21\n",
      "Run 1: Best Training Loss: 0.68926910832832, Best Validation Loss: 0.6895918794386081, Misclassification Error (Test): 0.39636363636363636\n",
      "Early stopping at epoch 54\n",
      "Run 2: Best Training Loss: 0.6893218925369369, Best Validation Loss: 0.6896426096567834, Misclassification Error (Test): 0.39636363636363636\n",
      "Early stopping at epoch 37\n",
      "Run 3: Best Training Loss: 0.6892907144768086, Best Validation Loss: 0.689618433175793, Misclassification Error (Test): 0.39636363636363636\n",
      "\n",
      "Training for n1=1, n2=3\n",
      "Early stopping at epoch 24\n",
      "Run 1: Best Training Loss: 0.6893494940717007, Best Validation Loss: 0.6896676476561315, Misclassification Error (Test): 0.39636363636363636\n",
      "Early stopping at epoch 43\n",
      "Run 2: Best Training Loss: 0.6892993592405772, Best Validation Loss: 0.6896108386880531, Misclassification Error (Test): 0.39636363636363636\n",
      "Early stopping at epoch 28\n",
      "Run 3: Best Training Loss: 0.6892624782364071, Best Validation Loss: 0.6895921016759458, Misclassification Error (Test): 0.39636363636363636\n",
      "\n",
      "Training for n1=1, n2=4\n",
      "Early stopping at epoch 37\n",
      "Run 1: Best Training Loss: 0.6893354234717352, Best Validation Loss: 0.6897222731084576, Misclassification Error (Test): 0.39636363636363636\n",
      "Early stopping at epoch 35\n",
      "Run 2: Best Training Loss: 0.6892894904651975, Best Validation Loss: 0.6896020199972098, Misclassification Error (Test): 0.39636363636363636\n",
      "Early stopping at epoch 23\n",
      "Run 3: Best Training Loss: 0.6894161264228853, Best Validation Loss: 0.6898273657187524, Misclassification Error (Test): 0.39636363636363636\n",
      "\n",
      "Training for n1=1, n2=5\n",
      "Early stopping at epoch 42\n",
      "Run 1: Best Training Loss: 0.6892947268484375, Best Validation Loss: 0.6896695770604507, Misclassification Error (Test): 0.39636363636363636\n",
      "Early stopping at epoch 49\n",
      "Run 2: Best Training Loss: 0.6892642958215093, Best Validation Loss: 0.6895912070420145, Misclassification Error (Test): 0.39636363636363636\n",
      "Early stopping at epoch 25\n",
      "Run 3: Best Training Loss: 0.6894442323188815, Best Validation Loss: 0.6898579885653974, Misclassification Error (Test): 0.39636363636363636\n",
      "\n",
      "Training for n1=1, n2=6\n",
      "Early stopping at epoch 23\n",
      "Run 1: Best Training Loss: 0.6901462497843535, Best Validation Loss: 0.6905670854739898, Misclassification Error (Test): 0.39636363636363636\n",
      "Early stopping at epoch 32\n",
      "Run 2: Best Training Loss: 0.68951281348063, Best Validation Loss: 0.6898318416935597, Misclassification Error (Test): 0.39636363636363636\n",
      "Early stopping at epoch 21\n",
      "Run 3: Best Training Loss: 0.6892589691610302, Best Validation Loss: 0.6895989184047873, Misclassification Error (Test): 0.39636363636363636\n",
      "\n",
      "Training for n1=1, n2=7\n",
      "Early stopping at epoch 22\n",
      "Run 1: Best Training Loss: 0.6901171680310326, Best Validation Loss: 0.6904165360326077, Misclassification Error (Test): 0.3890909090909091\n",
      "Early stopping at epoch 28\n",
      "Run 2: Best Training Loss: 0.6893586382414624, Best Validation Loss: 0.6896737669134333, Misclassification Error (Test): 0.39636363636363636\n",
      "Early stopping at epoch 33\n",
      "Run 3: Best Training Loss: 0.6897148507019927, Best Validation Loss: 0.6900442438277438, Misclassification Error (Test): 0.39636363636363636\n",
      "\n",
      "Training for n1=2, n2=1\n",
      "Early stopping at epoch 57\n",
      "Run 1: Best Training Loss: 0.689709048706997, Best Validation Loss: 0.69016978537981, Misclassification Error (Test): 0.39636363636363636\n",
      "Early stopping at epoch 24\n",
      "Run 2: Best Training Loss: 0.6922641514709089, Best Validation Loss: 0.6929019794486514, Misclassification Error (Test): 0.39636363636363636\n",
      "Early stopping at epoch 16\n",
      "Run 3: Best Training Loss: 0.6892869915764462, Best Validation Loss: 0.6896105393906308, Misclassification Error (Test): 0.39636363636363636\n",
      "\n",
      "Training for n1=2, n2=2\n",
      "Early stopping at epoch 35\n",
      "Run 1: Best Training Loss: 0.6893446518241609, Best Validation Loss: 0.6896590255901969, Misclassification Error (Test): 0.39636363636363636\n",
      "Early stopping at epoch 36\n",
      "Run 2: Best Training Loss: 0.6894815737924006, Best Validation Loss: 0.689778843449668, Misclassification Error (Test): 0.39636363636363636\n",
      "Early stopping at epoch 34\n",
      "Run 3: Best Training Loss: 0.6896327204484155, Best Validation Loss: 0.6899086057664137, Misclassification Error (Test): 0.39636363636363636\n",
      "\n",
      "Training for n1=2, n2=3\n",
      "Early stopping at epoch 35\n",
      "Run 1: Best Training Loss: 0.6904157538566336, Best Validation Loss: 0.6906369325232392, Misclassification Error (Test): 0.3527272727272727\n",
      "Early stopping at epoch 26\n",
      "Run 2: Best Training Loss: 0.6895226498959274, Best Validation Loss: 0.6898620655937615, Misclassification Error (Test): 0.39636363636363636\n",
      "Early stopping at epoch 22\n",
      "Run 3: Best Training Loss: 0.6897373775447576, Best Validation Loss: 0.6900459869558967, Misclassification Error (Test): 0.39636363636363636\n",
      "\n",
      "Training for n1=2, n2=4\n",
      "Early stopping at epoch 46\n",
      "Run 1: Best Training Loss: 0.6895454026392931, Best Validation Loss: 0.6897982777079472, Misclassification Error (Test): 0.39636363636363636\n",
      "Early stopping at epoch 25\n",
      "Run 2: Best Training Loss: 0.6895026076601288, Best Validation Loss: 0.6898550275253895, Misclassification Error (Test): 0.39636363636363636\n",
      "Early stopping at epoch 40\n",
      "Run 3: Best Training Loss: 0.6896606120716718, Best Validation Loss: 0.6900744693664495, Misclassification Error (Test): 0.39636363636363636\n",
      "\n",
      "Training for n1=2, n2=5\n",
      "Early stopping at epoch 29\n",
      "Run 1: Best Training Loss: 0.6894435369488456, Best Validation Loss: 0.6897117378726078, Misclassification Error (Test): 0.39636363636363636\n",
      "Early stopping at epoch 22\n",
      "Run 2: Best Training Loss: 0.6892814249278667, Best Validation Loss: 0.6896065045793716, Misclassification Error (Test): 0.39636363636363636\n",
      "Early stopping at epoch 23\n",
      "Run 3: Best Training Loss: 0.6904911350242667, Best Validation Loss: 0.6906791334742945, Misclassification Error (Test): 0.36363636363636365\n",
      "\n",
      "Training for n1=2, n2=6\n",
      "Early stopping at epoch 31\n",
      "Run 1: Best Training Loss: 0.6893488711748645, Best Validation Loss: 0.6897088579155541, Misclassification Error (Test): 0.39636363636363636\n",
      "Early stopping at epoch 22\n",
      "Run 2: Best Training Loss: 0.6918536425056226, Best Validation Loss: 0.6922095107724509, Misclassification Error (Test): 0.23272727272727273\n",
      "Early stopping at epoch 42\n",
      "Run 3: Best Training Loss: 0.6893930250226727, Best Validation Loss: 0.689737433491678, Misclassification Error (Test): 0.39636363636363636\n",
      "\n",
      "Training for n1=3, n2=1\n",
      "Early stopping at epoch 38\n",
      "Run 1: Best Training Loss: 0.6893382726038609, Best Validation Loss: 0.689657852466162, Misclassification Error (Test): 0.39636363636363636\n",
      "Early stopping at epoch 92\n",
      "Run 2: Best Training Loss: 0.6892694410768888, Best Validation Loss: 0.6895952438667444, Misclassification Error (Test): 0.39636363636363636\n",
      "Early stopping at epoch 50\n",
      "Run 3: Best Training Loss: 0.6892800520469347, Best Validation Loss: 0.6896351194044262, Misclassification Error (Test): 0.39636363636363636\n",
      "\n",
      "Training for n1=3, n2=2\n",
      "Early stopping at epoch 25\n",
      "Run 1: Best Training Loss: 0.6894446719630469, Best Validation Loss: 0.6897763483904263, Misclassification Error (Test): 0.39636363636363636\n",
      "Early stopping at epoch 30\n",
      "Run 2: Best Training Loss: 0.6904599441846317, Best Validation Loss: 0.6907030478238154, Misclassification Error (Test): 0.3527272727272727\n",
      "Early stopping at epoch 31\n",
      "Run 3: Best Training Loss: 0.6893840541397044, Best Validation Loss: 0.6897073336462166, Misclassification Error (Test): 0.39636363636363636\n",
      "\n",
      "Training for n1=3, n2=3\n",
      "Early stopping at epoch 21\n",
      "Run 1: Best Training Loss: 0.6919234619098559, Best Validation Loss: 0.6925262733006863, Misclassification Error (Test): 0.39636363636363636\n",
      "Early stopping at epoch 25\n",
      "Run 2: Best Training Loss: 0.6894562641874171, Best Validation Loss: 0.6897530803689697, Misclassification Error (Test): 0.39636363636363636\n",
      "Early stopping at epoch 21\n",
      "Run 3: Best Training Loss: 0.6893303933131066, Best Validation Loss: 0.6896486612671276, Misclassification Error (Test): 0.39636363636363636\n",
      "\n",
      "Training for n1=3, n2=4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 26\n",
      "Run 1: Best Training Loss: 0.6900773517822494, Best Validation Loss: 0.6903778787070626, Misclassification Error (Test): 0.36363636363636365\n",
      "Early stopping at epoch 29\n",
      "Run 2: Best Training Loss: 0.6897016373274646, Best Validation Loss: 0.6899711812213902, Misclassification Error (Test): 0.39636363636363636\n",
      "Early stopping at epoch 33\n",
      "Run 3: Best Training Loss: 0.6894999276067098, Best Validation Loss: 0.689800975018447, Misclassification Error (Test): 0.39636363636363636\n",
      "\n",
      "Training for n1=3, n2=5\n",
      "Early stopping at epoch 18\n",
      "Run 1: Best Training Loss: 0.6895649651389946, Best Validation Loss: 0.6899182884286494, Misclassification Error (Test): 0.39636363636363636\n",
      "Early stopping at epoch 29\n",
      "Run 2: Best Training Loss: 0.6897423574136831, Best Validation Loss: 0.6899688470107216, Misclassification Error (Test): 0.39636363636363636\n",
      "Early stopping at epoch 22\n",
      "Run 3: Best Training Loss: 0.6916873377686955, Best Validation Loss: 0.6919613265218041, Misclassification Error (Test): 0.2727272727272727\n",
      "\n",
      "Training for n1=4, n2=1\n",
      "Early stopping at epoch 41\n",
      "Run 1: Best Training Loss: 0.6906983314449266, Best Validation Loss: 0.6912450056472237, Misclassification Error (Test): 0.39636363636363636\n",
      "Early stopping at epoch 15\n",
      "Run 2: Best Training Loss: 0.6893250076346765, Best Validation Loss: 0.689640080989054, Misclassification Error (Test): 0.39636363636363636\n",
      "Early stopping at epoch 27\n",
      "Run 3: Best Training Loss: 0.710409382472315, Best Validation Loss: 0.709862249006034, Misclassification Error (Test): 0.6036363636363636\n",
      "\n",
      "Training for n1=4, n2=2\n",
      "Early stopping at epoch 29\n",
      "Run 1: Best Training Loss: 0.6895466461470984, Best Validation Loss: 0.6898639387044745, Misclassification Error (Test): 0.39636363636363636\n",
      "Early stopping at epoch 41\n",
      "Run 2: Best Training Loss: 0.6892780713884018, Best Validation Loss: 0.6896003681042328, Misclassification Error (Test): 0.39636363636363636\n",
      "Early stopping at epoch 32\n",
      "Run 3: Best Training Loss: 0.6893368113964807, Best Validation Loss: 0.6896477839206009, Misclassification Error (Test): 0.39636363636363636\n",
      "\n",
      "Training for n1=4, n2=3\n",
      "Early stopping at epoch 19\n",
      "Run 1: Best Training Loss: 0.6893088047811651, Best Validation Loss: 0.6896271152454102, Misclassification Error (Test): 0.39636363636363636\n",
      "Early stopping at epoch 37\n",
      "Run 2: Best Training Loss: 0.689594457878287, Best Validation Loss: 0.6898620979841589, Misclassification Error (Test): 0.39636363636363636\n",
      "Early stopping at epoch 18\n",
      "Run 3: Best Training Loss: 0.6901951275681331, Best Validation Loss: 0.6904026006018249, Misclassification Error (Test): 0.3490909090909091\n",
      "\n",
      "Training for n1=4, n2=4\n",
      "Early stopping at epoch 20\n",
      "Run 1: Best Training Loss: 0.6894936322079518, Best Validation Loss: 0.6898177853594724, Misclassification Error (Test): 0.39636363636363636\n",
      "Early stopping at epoch 26\n",
      "Run 2: Best Training Loss: 0.6898805230869053, Best Validation Loss: 0.6901832201199533, Misclassification Error (Test): 0.39636363636363636\n",
      "Early stopping at epoch 27\n",
      "Run 3: Best Training Loss: 0.6909041591703823, Best Validation Loss: 0.6911738134881626, Misclassification Error (Test): 0.2581818181818182\n",
      "\n",
      "Training for n1=5, n2=1\n",
      "Early stopping at epoch 49\n",
      "Run 1: Best Training Loss: 0.6945087347659725, Best Validation Loss: 0.6944570903864623, Misclassification Error (Test): 0.6036363636363636\n",
      "Early stopping at epoch 32\n",
      "Run 2: Best Training Loss: 0.6893117246335072, Best Validation Loss: 0.6896343141966554, Misclassification Error (Test): 0.39636363636363636\n",
      "Early stopping at epoch 28\n",
      "Run 3: Best Training Loss: 0.6913294153671348, Best Validation Loss: 0.6919364149298023, Misclassification Error (Test): 0.39636363636363636\n",
      "\n",
      "Training for n1=5, n2=2\n",
      "Early stopping at epoch 31\n",
      "Run 1: Best Training Loss: 0.6907367778972199, Best Validation Loss: 0.6912801896984351, Misclassification Error (Test): 0.39636363636363636\n",
      "Early stopping at epoch 35\n",
      "Run 2: Best Training Loss: 0.6894268752436881, Best Validation Loss: 0.6897460423888627, Misclassification Error (Test): 0.39636363636363636\n",
      "Early stopping at epoch 35\n",
      "Run 3: Best Training Loss: 0.6914746410332703, Best Validation Loss: 0.692077339436833, Misclassification Error (Test): 0.39636363636363636\n",
      "\n",
      "Training for n1=5, n2=3\n",
      "Early stopping at epoch 21\n",
      "Run 1: Best Training Loss: 0.6913754846230017, Best Validation Loss: 0.6917868005570832, Misclassification Error (Test): 0.3490909090909091\n",
      "Early stopping at epoch 26\n",
      "Run 2: Best Training Loss: 0.6902820703908382, Best Validation Loss: 0.6907873290108241, Misclassification Error (Test): 0.39636363636363636\n",
      "Early stopping at epoch 24\n",
      "Run 3: Best Training Loss: 0.6905809322522819, Best Validation Loss: 0.6908276025597184, Misclassification Error (Test): 0.29818181818181816\n",
      "\n",
      "Training for n1=6, n2=1\n",
      "Early stopping at epoch 15\n",
      "Run 1: Best Training Loss: 0.6892710431816382, Best Validation Loss: 0.6896211203198929, Misclassification Error (Test): 0.39636363636363636\n",
      "Early stopping at epoch 82\n",
      "Run 2: Best Training Loss: 0.6892913167462634, Best Validation Loss: 0.6896154818645281, Misclassification Error (Test): 0.39636363636363636\n",
      "Early stopping at epoch 42\n",
      "Run 3: Best Training Loss: 0.6914669782142367, Best Validation Loss: 0.6920509347140205, Misclassification Error (Test): 0.39636363636363636\n",
      "\n",
      "Training for n1=6, n2=2\n",
      "Early stopping at epoch 24\n",
      "Run 1: Best Training Loss: 0.6894370753792545, Best Validation Loss: 0.6897865525130935, Misclassification Error (Test): 0.39636363636363636\n",
      "Early stopping at epoch 26\n",
      "Run 2: Best Training Loss: 0.6898212588833988, Best Validation Loss: 0.6902412208891155, Misclassification Error (Test): 0.39636363636363636\n",
      "Early stopping at epoch 32\n",
      "Run 3: Best Training Loss: 0.6896042468263648, Best Validation Loss: 0.6898856885652421, Misclassification Error (Test): 0.39636363636363636\n",
      "\n",
      "Training for n1=7, n2=1\n",
      "Early stopping at epoch 73\n",
      "Run 1: Best Training Loss: 0.6895448549236272, Best Validation Loss: 0.6898363733799833, Misclassification Error (Test): 0.39636363636363636\n",
      "Run 2: Best Training Loss: 0.6892832949641176, Best Validation Loss: 0.6896061553742462, Misclassification Error (Test): 0.39636363636363636\n",
      "Early stopping at epoch 15\n",
      "Run 3: Best Training Loss: 0.6915943076683385, Best Validation Loss: 0.6922032161359931, Misclassification Error (Test): 0.39636363636363636\n",
      "\n",
      "Best Network Configuration: n1=1, n2=5\n",
      "Best Validation Loss: 0.6895912070420145\n",
      "Best Training Loss: 0.6892642958215093\n",
      "Best Misclassification Error (Test): 0.39636363636363636\n",
      "Early stopping at epoch 15\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZsAAAEGCAYAAACzYDhlAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dfZwV1Z3n8c+3u1UelEgENwoqsAMaBHlq0UWJGB2D0UGjsoo4xrAbg9E4wYmRzGo0cbPJZNiYYaNhCFGT0cBkDD5NFHzYKLOJk9gNqBAwi8hgoxMbZ1UkKnb3b/+oc5vbt283DXZxG/N9v173dat+dc6pUzR9f32q6p5SRGBmZpanqkp3wMzMPvicbMzMLHdONmZmljsnGzMzy52TjZmZ5a6m0h3oqQYMGBBDhgypdDfMzPYp9fX1WyNiYGncyaYDQ4YMoa6urtLdMDPbp0j613Jxn0YzM7PcOdmYmVnuck02kqZKel7SBklzOygzRdJqSWslPVkUn5NiayQtltQrxcdIekrSc5IelNSvqM5xadvatL1QZ0Ja3yBpviTledxmZtZWbslGUjVwK3AmMBKYIWlkSZmDgduAaRFxLDA9xQcBVwO1ETEKqAYuStUWAXMjYjRwL3BtqlMD3AXMTm1NAd5Ldb4PXA4MT6+pORyymZl1IM+RzURgQ0RsjIgdwBLgnJIyFwNLI2IzQES8WrStBuidkkgf4OUUPxpYkZYfBc5Py2cAz0bEM6mt1yKiWdJhQL+IeCqyieB+DJzbnQdqZmadyzPZDAJeKlpvSLFiI4D+kp6QVC/pUoCI2ALMAzYDrwBvRMQjqc4aYFpang4cUdRWSFouaaWkLxf1o2EX/QBA0uWS6iTVNTY27ubhmplZR/JMNuWui5ROMV0DTADOAj4B3CBphKT+ZKOgocDhQF9Jl6Q6s4ArJdUDBwE7ito6GZiZ3j8l6bQu9iMLRiyMiNqIqB04sN1t4mZmtofy/J5NAztHHQCD2XkqrLjM1ojYDmyXtAIYk7a9GBGNAJKWApOAuyJiPdkpMySNIEtUhbaejIitadtDwHiy6ziDd9EPM7PKaGmBliaIZmhpTssp1tKc4mm5db25pE6hTEk7xfVatxXiLUXLJfVOmQvV3Zse8kw2TwPDJQ0FtpBd4L+4pMz9wPfSdZn9gROAW4C+wImS+gBvA6cBdQCSDo2IVyVVAdcDC1Jby4Evpzo7gFOAWyLiFUnbJJ0I/Bq4FPhfeR202T4pIr1auvZqaS5aT8stLe1jrWWjpGxzB3WbS7bFzg/Y1nq7qNNSLlZSv8PyRe/F+2794C9to3nnB3dpG+3KFeqXJJEeRzD5L/edZBMRTZKuIksC1cDtEbFW0uy0fUFErJO0DHgWaAEWRcQaAEn3ACuBJmAVsDA1PUPSlWl5KXBHau//SfoOWZIL4KGI+HkqdwVwJ9AbeDi98vHiCnj3rS4U3I2H1rV5wF2UiZeLdSEeHbRV2u4ut5Usdxijg1h0HIuWMtsjNV8S77BsaRnal9/lMh3EWzpY76BMuXIUbSv7gV9aplzZcu13Ur5Nsmhp+3/lg0TVoCqoqs6Wq0rW22yrKipT2FaI1ZS0sV/2XlVTvt3WbaX7Lmwrqdu6rWYX5Qpt1rSvU/reuq3wqtq53FG9wr9DHj8KP6mzvNra2tij6WpuPQEa13d/h6yLBFIH71UdLHelTJnyqmq7rc16VXa1sF2so3oUlSl9lZRt12ZV+30WyhQ+BMu2V25byX5KX1XV5euX20+bWPXOtkvjpXXbJYLS9jraViZ52F4nqT4iakvjnhutu02/E5re7VrZ3fpuaVHZNvXUcaxLcbVdLq1TdltpOZVZ7mqseD8qab+qfbyzd39X16zHcrLpbod+tNI9MDPrcTzONDOz3DnZmJlZ7pxszMwsd042ZmaWOycbMzPLnZONmZnlzsnGzMxy52RjZma5c7IxM7PcOdmYmVnunGzMzCx3TjZmZpY7JxszM8udk42ZmeXOycbMzHLnZGNmZrlzsjEzs9w52ZiZWe6cbMzMLHe5JhtJUyU9L2mDpLkdlJkiabWktZKeLIrPSbE1khZL6pXiYyQ9Jek5SQ9K6pfiQyS9ndpaLWlBUVtPpH4Uth2a53GbmVlbuSUbSdXArcCZwEhghqSRJWUOBm4DpkXEscD0FB8EXA3URsQooBq4KFVbBMyNiNHAvcC1RU2+EBFj02t2SZdmFm17tVsP1szMOpXnyGYisCEiNkbEDmAJcE5JmYuBpRGxGaAkCdQAvSXVAH2Al1P8aGBFWn4UOD+n/puZWTfJM9kMAl4qWm9IsWIjgP7pNFe9pEsBImILMA/YDLwCvBERj6Q6a4BpaXk6cERRe0MlrZL0pKTJJfu6I51Cu0GSynVY0uWS6iTVNTY27ubhmplZR/JMNuU+0KNkvQaYAJwFfAK4QdIISf3JRkFDgcOBvpIuSXVmAVdKqgcOAnak+CvAkRExDrgG+Enheg7ZKbTRwOT0+vNyHY6IhRFRGxG1AwcO3P0jNjOzsvJMNg20HXUMZuepsOIyyyJie0RsJTs9NgY4HXgxIhoj4j1gKTAJICLWR8QZETEBWAy8kOLvRsRrabk+xUek9S3pfRvwE7JTfGZmtpfkmWyeBoZLGippf7IL/A+UlLkfmCypRlIf4ARgHdnpsxMl9UmnvE5LcQp3kkmqAq4HFqT1gemmBCQNA4YDG1PbA1J8P+BsslNxZma2l9Tk1XBENEm6ClhOdjfZ7RGxVtLstH1BRKyTtAx4FmgBFkXEGgBJ9wArgSZgFbAwNT1D0pVpeSlwR1r+GPB1SU1AMzA7Iv5dUl9geUo01cBjwA/yOm4zM2tPEaWXUQygtrY26urqKt0NM7N9iqT6iKgtjXsGATMzy52TjZmZ5c7JxszMcudkY2ZmuXOyMTOz3DnZmJlZ7pxszMwsd042ZmaWOycbMzPLnZONmZnlzsnGzMxy52RjZma5c7IxM7PcOdmYmVnunGzMzCx3TjZmZpY7JxszM8udk42ZmeXOycbMzHLnZGNmZrlzsjEzs9zlmmwkTZX0vKQNkuZ2UGaKpNWS1kp6sig+J8XWSFosqVeKj5H0lKTnJD0oqV+KD5H0dmprtaQFRW1NSOU3SJovSXket5mZtZVbspFUDdwKnAmMBGZIGllS5mDgNmBaRBwLTE/xQcDVQG1EjAKqgYtStUXA3IgYDdwLXFvU5AsRMTa9ZhfFvw9cDgxPr6nderBmZtapPEc2E4ENEbExInYAS4BzSspcDCyNiM0AEfFq0bYaoLekGqAP8HKKHw2sSMuPAud31glJhwH9IuKpiAjgx8C5e35YZma2u/JMNoOAl4rWG1Ks2Aigv6QnJNVLuhQgIrYA84DNwCvAGxHxSKqzBpiWlqcDRxS1N1TSKklPSppc1I+GXfQDAEmXS6qTVNfY2Lg7x2pmZp3IM9mUuy4SJes1wATgLOATwA2SRkjqTzYKGgocDvSVdEmqMwu4UlI9cBCwI8VfAY6MiHHANcBP0vWcrvQjC0YsjIjaiKgdOHBgV4/TzMx2oSbHthtoO+oYzM5TYcVltkbEdmC7pBXAmLTtxYhoBJC0FJgE3BUR64EzUnwEWaIiIt4F3k3L9ZJeIBs5NaR9d9YPMzPLUZ4jm6eB4ZKGStqf7AL/AyVl7gcmS6qR1Ac4AVhHdvrsREl90p1jp6U4kg5N71XA9cCCtD4w3ZSApGFkNwJsjIhXgG2STkxtXZr2a2Zme0luI5uIaJJ0FbCc7G6y2yNiraTZafuCiFgnaRnwLNACLIqINQCS7gFWAk3AKmBhanqGpCvT8lLgjrT8MeDrkpqAZmB2RPx72nYFcCfQG3g4vczMbC9RdoOWlaqtrY26urpKd8PMbJ8iqT4iakvjnkHAzMxy52RjZma5c7IxM7PcOdmYmVnunGzMzCx3TjZmZpY7JxszM8udk42ZmeXOycbMzHLnZGNmZrlzsjEzs9w52ZiZWe6cbMzMLHdONmZmljsnGzMzy52TjZmZ5c7JxszMcudkY2ZmuXOyMTOz3DnZmJlZ7moq3QEz++P23nvv0dDQwDvvvFPprthu6NWrF4MHD2a//fbrUvlck42kqcDfAtXAooj4VpkyU4DvAvsBWyPilBSfA/xXIIDngM9ExDuSxgALgAOBTcDMiHizqL0jgd8CN0XEvBR7AjgMeDsVOyMiXu3u4zWz3dfQ0MBBBx3EkCFDkFTp7lgXRASvvfYaDQ0NDB06tEt1cjuNJqkauBU4ExgJzJA0sqTMwcBtwLSIOBaYnuKDgKuB2ogYRZasLkrVFgFzI2I0cC9wbcmubwEeLtOlmRExNr2caMx6iHfeeYdDDjnEiWYfIolDDjlkt0ajeV6zmQhsiIiNEbEDWAKcU1LmYmBpRGwGKEkCNUBvSTVAH+DlFD8aWJGWHwXOL1SQdC6wEVjbzcdiZjlyotn37O7PLM9kMwh4qWi9IcWKjQD6S3pCUr2kSwEiYgswD9gMvAK8ERGPpDprgGlpeTpwBICkvsB1wNc66M8dklZLukEd/CtJulxSnaS6xsbG3TlWM9tHvfbaa4wdO5axY8fykY98hEGDBrWu79ixo9O6dXV1XH311bvcx6RJk7qlr0888QRnn312t7S1t+V5zabcB3qU2f8E4DSgN/CUpH8BGslGQUOB14F/lHRJRNwFzALmS/oq8ABQ+N/wNeCWiHirTC6ZGRFbJB0E/Az4c+DH7ToXsRBYCFBbW1vaVzP7ADrkkENYvXo1ADfddBMHHnggX/rSl1q3NzU1UVNT/qOytraW2traXe7jV7/6Vfd0dh+W58imgTTqSAaz81RYcZllEbE9IraSnR4bA5wOvBgRjRHxHrAUmAQQEesj4oyImAAsBl5IbZ0AfFvSJuCLwF9JuirV2ZLetwE/ITvFZ2ZW1mWXXcY111zDqaeeynXXXcdvfvMbJk2axLhx45g0aRLPP/880HakcdNNNzFr1iymTJnCsGHDmD9/fmt7Bx54YGv5KVOmcMEFF3DMMccwc+ZMIrK/ax966CGOOeYYTj75ZK6++urdGsEsXryY0aNHM2rUKK677joAmpubueyyyxg1ahSjR4/mlltuAWD+/PmMHDmS4447josuuqizZrtVniObp4HhkoYCW8gu8F9cUuZ+4Hvpusz+ZAnjFqAvcKKkPmR3kJ0G1AFIOjQiXpVUBVxPdmcaETG50Kikm4C3IqLQ9sERsVXSfsDZwGM5HbOZvQ9fe3Atv335zV0X3A0jD+/HjX927G7X+93vfsdjjz1GdXU1b775JitWrKCmpobHHnuMv/qrv+JnP/tZuzrr16/nF7/4Bdu2bePoo4/miiuuaHdr8KpVq1i7di2HH344J510Er/85S+pra3lc5/7HCtWrGDo0KHMmDGjy/18+eWXue6666ivr6d///6cccYZ3HfffRxxxBFs2bKFNWvWAPD6668D8K1vfYsXX3yRAw44oDW2N3RpZCOpb/pwR9IISdPSB3eHIqIJuApYDqwDfhoRayXNljQ7lVkHLAOeBX5Ddnv0moj4NXAPsJLstucq0uktsrvafgesJxsp3bGL7h8ALJf0LLCaLPH9oCvHbWZ/vKZPn051dTUAb7zxBtOnT2fUqFHMmTOHtWvL34N01llnccABBzBgwAAOPfRQfv/737crM3HiRAYPHkxVVRVjx45l06ZNrF+/nmHDhrXeRrw7yebpp59mypQpDBw4kJqaGmbOnMmKFSsYNmwYGzdu5Atf+ALLli2jX79+ABx33HHMnDmTu+66q8PTg3no6p5WAJMl9QceJxtlXAjM7KxSRDwEPFQSW1Cy/jfA35SpeyNwY5n435J9d6ez/d5UtLyd7LqQmfVwezICyUvfvn1bl2+44QZOPfVU7r33XjZt2sSUKVPK1jnggANal6urq2lqaupSmcKptD3RUd3+/fvzzDPPsHz5cm699VZ++tOfcvvtt/Pzn/+cFStW8MADD3DzzTezdu3avZJ0unrNRhHxB+A84H9FxKfIvjtjZvaB98YbbzBoUHYz7Z133tnt7R9zzDFs3LiRTZs2AfAP//APXa57wgkn8OSTT7J161aam5tZvHgxp5xyClu3bqWlpYXzzz+fm2++mZUrV9LS0sJLL73Eqaeeyre//W1ef/113nrrrW4/nnK6ms4k6T+RjWT+y27WNTPbp335y1/m05/+NN/5znf4+Mc/3u3t9+7dm9tuu42pU6cyYMAAJk7s+B6mxx9/nMGDB7eu/+M//iPf/OY3OfXUU4kIPvnJT3LOOefwzDPP8JnPfIaWlhYAvvnNb9Lc3Mwll1zCG2+8QUQwZ84cDj744G4/nnLUleGbpFOAvwR+GRF/LWkY8MWI2PUN5vuo2traqKurq3Q3zD7w1q1bx0c/+tFKd6Pi3nrrLQ488EAigiuvvJLhw4czZ86cSnerU+V+dpLqI6Ld/eBdGp1ExJPAk6mhKrI5zD6wicbMbG/7wQ9+wI9+9CN27NjBuHHj+NznPlfpLnWrLiUbST8BZgPNQD3wIUnfSRf3zczsfZozZ06PH8m8H129QWBkmln5XLK7y44k+xa+mZnZLnU12eyXvldzLnB/+la/p3MxM7Mu6Wqy+TuyZ8f0BVZIOgro3q/5mpnZB1ZXbxCYD8wvCv2rpFPz6ZKZmX3QdHW6mg9J+k5h+n1J/5NslGNmtk+bMmUKy5cvbxP77ne/y+c///lO6xS+GvHJT36y7BxjN910E/Pmzet03/fddx+//e1vW9e/+tWv8thj73/qxp74KIKunka7HdgG/Of0epNdz0lmZtbjzZgxgyVLlrSJLVmypMvzkz300EN7/MXI0mTz9a9/ndNPP32P2urpupps/mNE3JieurkxIr4GDMuzY2Zme8MFF1zAP/3TP/Huu+8CsGnTJl5++WVOPvlkrrjiCmprazn22GO58cZ2UzUCMGTIELZu3QrAN77xDY4++mhOP/301scQQPYdmuOPP54xY8Zw/vnn84c//IFf/epXPPDAA1x77bWMHTuWF154gcsuu4x77rkHyGYKGDduHKNHj2bWrFmt/RsyZAg33ngj48ePZ/To0axfv77Lx1rJRxF0dcqZtyWdHBH/B0DSSWRT/5uZdZ+H58K/Pde9bX5kNJz5rQ43H3LIIUycOJFly5ZxzjnnsGTJEi688EIk8Y1vfIMPf/jDNDc3c9ppp/Hss89y3HHHlW2nvr6eJUuWsGrVKpqamhg/fjwTJmRzAJ933nl89rOfBeD666/nhz/8IV/4wheYNm0aZ599NhdccEGbtt555x0uu+wyHn/8cUaMGMGll17K97//fb74xS8CMGDAAFauXMltt93GvHnzWLRo0S7/GSr9KIKujmxmA7dK2pQeTvY94IP19VYz+6NVfCqt+BTaT3/6U8aPH8+4ceNYu3Ztm1Nepf75n/+ZT33qU/Tp04d+/foxbdq01m1r1qxh8uTJjB49mrvvvrvDRxQUPP/88wwdOpQRI0YA8OlPf5oVK1a0bj/vvPMAmDBhQuvknbtS6UcRdPVutGeAMZL6pfU3JX2R7Dk0Zmbdo5MRSJ7OPfdcrrnmGlauXMnbb7/N+PHjefHFF5k3bx5PP/00/fv357LLLuOdd97ptJ0yj6QHsid/3nfffYwZM4Y777yTJ554otN2djVnZeExBR09xmB32txbjyLYrcdCR8SbaSYBgGv2eK9mZj3IgQceyJQpU5g1a1brqObNN9+kb9++fOhDH+L3v/89Dz/8cKdtfOxjH+Pee+/l7bffZtu2bTz44IOt27Zt28Zhhx3Ge++9x913390aP+igg9i2bVu7to455hg2bdrEhg0bAPj7v/97TjnllPd1jJV+FMH7GRuVT+FmZvugGTNmcN5557WeThszZgzjxo3j2GOPZdiwYZx00kmd1h8/fjwXXnghY8eO5aijjmLy5NYn1XPzzTdzwgkncNRRRzF69OjWBHPRRRfx2c9+lvnz57feGADQq1cv7rjjDqZPn05TUxPHH388s2fP3q3j6WmPIujSIwbKVpQ2R8SR72vvPZgfMWC2d/gRA/uubnvEgKRtlJ8DTUDv99NJMzP749FpsomIg/ZWR8zM7INrt24QMDMz2xO5JhtJUyU9L2mDpLkdlJkiabWktZKeLIrPSbE1khZL6pXiYyQ9Jek5SQ8WbscuqnekpLckfakoNiGV3yBpvjq6P9HMKmJPrx1b5ezuzyy3ZCOpGrgVOBMYCcyQNLKkzMHAbcC0iDgWmJ7ig4CrgdqIGAVUA4X5EhYBcyNiNHAvcG3Jrm8BSu9R/D5wOTA8vaZ2xzGa2fvXq1cvXnvtNSecfUhE8Nprr9GrV68u13n/Xwvt2ERgQ0RsBJC0BDgHKP4K7sXA0ojYDBARr5b0rbek94A+wMspfjRQ+Crto8By4Ia0j3OBjcD2QiOSDgP6RcRTaf3HZA+B6/ymeTPbKwYPHkxDQwONjY2V7orthl69erW5tXpX8kw2g4CXitYbgBNKyowgewroE8BBwN9GxI8jYoukecBmsjnYHomIR1KdNcA04H6ykdARAJL6AtcBfwp8qWgfg9K+i/sxqFyHJV1ONgLiyCM/sHd1m/Uo++23H0OHDq10NyxneV6zKXddpHScXANMAM4CPgHcIGmEpP5ko6ChwOFAX0mXpDqzgCsl1ZMlqB0p/jXglogo/ZprV/qRBSMWRkRtRNQOHDiw86MzM7Muy3Nk00AadSSD2XkqrLjM1ojYDmyXtAIYk7a9GBGNAJKWApOAuyJiPXBGio8gS1SQjZoukPRt4GCgRdI7wM/Svjvrh5mZ5SjPkc3TwHBJQyXtT3aB/4GSMvcDkyXVSOpDljDWkZ0+O1FSn3Tn2GkpjqRD03sVcD2wACAiJkfEkIgYAnwX+B8R8b2IeAXYJunE1Nalab9mZraX5DayiYgmSVeRXcCvBm6PiLWSZqftCyJinaRlZLNHtwCLImINgKR7gJVAE7AKWJianiHpyrS8lK49MfQK4E6yWQ8exjcHmJntVXs8N9oHnedGMzPbfR3NjeYZBMzMLHdONmZmljsnGzMzy52TjZmZ5c7JxszMcudkY2ZmuXOyMTOz3DnZmJlZ7pxszMwsd042ZmaWOycbMzPLnZONmZnlzsnGzMxy52RjZma5c7IxM7PcOdmYmVnunGzMzCx3TjZmZpY7JxszM8udk42ZmeUu12Qjaaqk5yVtkDS3gzJTJK2WtFbSk0XxOSm2RtJiSb1SfIykpyQ9J+lBSf1SfGJqZ7WkZyR9qqitJ1I/CtsPzfO4zcysrdySjaRq4FbgTGAkMEPSyJIyBwO3AdMi4lhgeooPAq4GaiNiFFANXJSqLQLmRsRo4F7g2hRfk8qPBaYCfyeppmh3MyNibHq92v1HbGZmHclzZDMR2BARGyNiB7AEOKekzMXA0ojYDFCSBGqA3ilh9AFeTvGjgRVp+VHg/FT3DxHRlOK9gOjm4zEzsz2UZ7IZBLxUtN6QYsVGAP3Taa56SZcCRMQWYB6wGXgFeCMiHkl11gDT0vJ04IhCY5JOkLQWeA6YXZR8AO5Ip9BukKTuOUQzM+uKPJNNuQ/00tFGDTABOAv4BHCDpBGS+pONgoYChwN9JV2S6swCrpRUDxwE7GhtPOLX6XTc8cBXCtd5yE6hjQYmp9efl+2wdLmkOkl1jY2Nu3/EZmZWVp7JpoGiUQcwmJ2nworLLIuI7RGxlez02BjgdODFiGiMiPeApcAkgIhYHxFnRMQEYDHwQumOI2IdsB0Ylda3pPdtwE/ITvG1ExELI6I2ImoHDhy4h4dtZmal8kw2TwPDJQ2VtD/ZBf4HSsrcD0yWVCOpD3ACsI7s9NmJkvqkU16npTiFO8kkVQHXAwvS+tDCDQGSjiK7trMptT0gxfcDziY7FWdmZntJza6L7JmIaJJ0FbCc7G6y2yNiraTZafuCiFgnaRnwLNACLIqINQCS7gFWAk3AKmBhanqGpCvT8lLgjrR8MjBX0nuprc9HxFZJfYHlKdFUA48BP8jruM3MrD1F+Katcmpra6Ourq7S3TAz26dIqo+I2tK4ZxAwM7PcOdmYmVnunGzMzCx3TjZmZpY7JxszM8udk42ZmeXOycbMzHLnZGNmZrlzsjEzs9w52ZiZWe6cbMzMLHdONmZmljsnGzMzy52TjZmZ5c7JxszMcudkY2ZmuXOyMTOz3DnZmJlZ7pxszMwsd042ZmaWOycbMzPLXa7JRtJUSc9L2iBpbgdlpkhaLWmtpCeL4nNSbI2kxZJ6pfgYSU9Jek7Sg5L6pfjE1M5qSc9I+lRRWxNS+Q2S5ktSnsdtZmZt5ZZsJFUDtwJnAiOBGZJGlpQ5GLgNmBYRxwLTU3wQcDVQGxGjgGrgolRtETA3IkYD9wLXpviaVH4sMBX4O0k1adv3gcuB4ek1tfuP2MzMOpLnyGYisCEiNkbEDmAJcE5JmYuBpRGxGSAiXi3aVgP0TgmjD/Byih8NrEjLjwLnp7p/iIimFO8FBICkw4B+EfFURATwY+Dc7jtMMzPblTyTzSDgpaL1hhQrNgLoL+kJSfWSLgWIiC3APGAz8ArwRkQ8kuqsAaal5enAEYXGJJ0gaS3wHDA7JZ9Bad+d9cPMzHKUZ7Ipd10kStZrgAnAWcAngBskjZDUn2wUNBQ4HOgr6ZJUZxZwpaR64CBgR2vjEb9Op+OOB76SrvN0pR9Zh6XLJdVJqmtsbOzqcZqZ2S7U7LrIHmugaNQBDGbnqbDiMlsjYjuwXdIKYEza9mJENAJIWgpMAu6KiPXAGSk+gixRtRER6yRtB0alfQzeRT8K9RYCCwFqa2vLJiQzM9t9eY5sngaGSxoqaX+yC/wPlJS5H5gsqUZSH+AEYB3Z6bMTJfVJd46dluJIOjS9VwHXAwvS+tDCDQGSjiK7trMpIl4Btkk6MbV1adqvmZntJbmNbCKiSdJVwHKyu8luj4i1kman7QvSCGQZ8CzQAiyKiDUAku4BVgJNwCrSiIPsrrYr0/JS4I60fDIwV9J7qa3PR8TWtO0K4E6gN/BwepmZ2V6i7AYtK1VbWxt1dXWV7oaZ2T5FUn1E1JbGPYOAmZnlztfGFfkAAArWSURBVMnGzMxy52RjZma5c7IxM7PcOdmYmVnunGzMzCx3TjZmZpY7JxszM8udk42ZmeXOycbMzHLnZGNmZrlzsjEzs9w52ZiZGRFBU3ML77zXTB4TNOf58DQzsx4pImhuCZoL7y1BSwut6y1F8UK5lpagqSTWpkxXtkXWxs62WrL35p11Cvtoas760dTS0rpe3EZzc0kbbeq2j73X3FK2TPExFTz/36dyQE11t/6bO9mYlYgIIrJnh0dEeocgxYuXgZZC+fTeUlwnLbcUb0u/0y3tyhfWs1hpnSyWlSv33lontdXS0r5OoUxL0fbitrP14rKFukFzob2WLN7cWjbbVvgAL95P63rhA7xof8Uf6oUyhQ/cCNp8yGd1U5mWaN9+Ybm1jSzW1NyS9bWorUIfeqIqQU1VFdVVan3VFL1XtVmval2vqd4Z33+/aqoPqGG/6p3l2rRTXdWmzepqsV9xmWpRJXX7sTnZdLMZC/+FF7duB0CCwo9MJT+8wmrreyq5c71tvbI/+jLBcuVK911OuWFz2d/HMsHiUKGdaF0vlImiMm3fy/Wh3Qd8m7ptk0Gb8unDljZ1dp0wistY10lQrezDqaoKqiSqpSxeVYiLqlROUorTuq24fKFOcZn9a7IPQklUF7dbKFeVxYvbK/5QzsrRuq14H60v7WxnZ5tt26kuqt/u1UG8Ju0rSwwUxdsmgOI+fFA52XSz44f058gP92n9IINOPnjbbe/og7q995ccojW5tfE+kpfabG8bK5swd5FkCzGRfRBl21UUb7u9sLV1X2W3ZdvbtNtuP2of66RuldrWqyou1xrL1qtSXdIHo4Cqqrb7bv0Qbl0vKlf4UE/tVBXXSR/OsLMNtba1MxEUt1v4kC/UbVenXZ/aJ5Su/CFjBk423e6aM46udBfMzHoc341mZma5c7IxM7PcOdmYmVnuck02kqZKel7SBklzOygzRdJqSWslPVkUn5NiayQtltQrxcdIekrSc5IelNQvxf9UUn2K10v6eFFbT6R+rE6vQ/M8bjMzayu3ZCOpGrgVOBMYCcyQNLKkzMHAbcC0iDgWmJ7ig4CrgdqIGAVUAxelaouAuRExGrgXuDbFtwJ/luKfBv6+pEszI2Jser3avUdrZmadyXNkMxHYEBEbI2IHsAQ4p6TMxcDSiNgMUJIEaoDekmqAPsDLKX40sCItPwqcn+quiohCmbVAL0kHdPMxmZnZHsgz2QwCXipab0ixYiOA/uk0V72kSwEiYgswD9gMvAK8ERGPpDprgGlpeTpwRJl9nw+sioh3i2J3pFNoN6iDLwdIulxSnaS6xsbGrh+pmZl1Ks9kU+4DvfQrhjXABOAs4BPADZJGSOpPNgoaChwO9JV0SaozC7hSUj1wELCjzU6lY4G/Bj5XFJ6ZTq9NTq8/L9fhiFgYEbURUTtw4MCuH6mZmXUqzy91NtB21DGYnafCistsjYjtwHZJK4AxaduLEdEIIGkpMAm4KyLWA2ek+AiyREVaH0x2HefSiHihEE8jJSJim6SfkJ3i+3Fnna+vr98q6V9375BbDSC7htST9fQ+9vT+Qc/vY0/vH/T8Pvb0/kHP6+NR5YJ5JpungeGShgJbyC7wX1xS5n7ge+m6zP7ACcAtQF/gREl9gLeB04A6AEmHRsSrkqqA64EFKX4w8HPgKxHxy8IOUtsHR8RWSfsBZwOP7arzEbHHQxtJdRFRu6f194ae3see3j/o+X3s6f2Dnt/Hnt4/2Df6CDkmm4hoknQVsJzsbrLbI2KtpNlp+4KIWCdpGfAs0AIsiog1AJLuAVYCTcAqYGFqeoakK9PyUuCOtHwV8Cdkp+JuSLEzgO3A8pRoqskSzQ/yOm4zM2tPeTwk54/dvvCXRk/vY0/vH/T8Pvb0/kHP72NP7x/sG30EzyCQl4W7LlJxPb2PPb1/0PP72NP7Bz2/jz29f7Bv9NEjGzMzy59HNmZmljsnGzMzy52TTTfqysSjlSTpCEm/kLQuTXL6F5XuUzmSqiWtkvRPle5LOZIOlnSPpPXp3/I/VbpPpTqayLbCfbpd0quS1hTFPizpUUn/N73372H9+5v0c35W0r3pKxYVU66PRdu+JCkkDahE33bFyaabdGXi0R6gCfjLiPgocCLZTAw9rY8AfwGsq3QnOvG3wLKIOIbsS8g9qq+7mMi2ku4EppbE5gKPR8Rw4PG0Xil30r5/jwKjIuI44HfAV/Z2p0rcSfs+IukI4E/JpvjqkZxsuk9XJh6tqIh4JSJWpuVtZB+SpfPVVVSaBeIsstm9e5z0SIuPAT8EiIgdEfF6ZXtVVkcT2VZMRKwA/r0kfA7wo7T8I+DcvdqpIuX6FxGPRERTWv0XsplQKqaDf0PIvgz/ZdpPCdZjONl0n65MPNpjSBoCjAN+XdmetPNdsl+alkp3pAPDgEayiV1XSVokqW+lO1VsFxPZ9jT/ISJegeyPIaAnP2tqFvBwpTtRStI0YEtEPFPpvnTGyab7dGXi0R5B0oHAz4AvRsSble5PgaSzgVcjor7SfelEDTAe+H5EjCOboaJHXZ/bxUS2tgck/Tey09B3V7ovxdKUXv8N+Gql+7IrTjbdpysTj1ZcmrbnZ8DdEbG00v0pcRIwTdImstOQH5d0V2W71E4D0BARhRHhPWTJpyc5nTSRbUS8Rzat06QK96kjv5d0GEB673EPNpT0abI5FWdGz/ti4n8k+6PimfR7MxhYKekjFe1VGU423ad14lFJ+5NdkH2gwn1qIz3H54fAuoj4TqX7UyoivhIRgyNiCNm/3/+OiB71F3lE/BvwkqSjU+g04LcV7FI5m0kT2aaf+Wn0sJsYijxA9mRd0vv9FexLO5KmAteRPU34D5XuT6mIeC4iDo2IIen3pgEYn/6f9ihONt0kXUQsTDy6DvhpRKytbK/aOYnsWT4fV/YgudWSPlnpTu2DvgDcLelZYCzwPyrcnzbSqKswke1zZL/nFZ/SRNJi4CngaEkNkv4L8C3gTyX9X7K7qb7Vw/r3PbLnZj2afl8WVKp/nfRxn+DpaszMLHce2ZiZWe6cbMzMLHdONmZmljsnGzMzy52TjZmZ5c7JxmwvktRcdNv56u6cHVzSkHKzAZv1BDWV7oDZH5m3I2JspTthtrd5ZGPWA0jaJOmvJf0mvf4kxY+S9Hh6nsrjko5M8f+Qnq/yTHoVpqOplvSD9CybRyT1TuWvlvTb1M6SCh2m/RFzsjHbu3qXnEa7sGjbmxExkexb699Nse8BP07PU7kbmJ/i84EnI2IM2dxshdkqhgO3RsSxwOvA+Sk+FxiX2pmd18GZdcQzCJjtRZLeiogDy8Q3AR+PiI1pstR/i4hDJG0FDouI91L8lYgYIKkRGBwR7xa1MQR4ND2IDEnXAftFxH+XtAx4C7gPuC8i3sr5UM3a8MjGrOeIDpY7KlPOu0XLzey8LnsW2ZNkJwD16aFqZnuNk41Zz3Fh0ftTaflX7Hyk80zg/6Tlx4ErIHskeXqCaFmSqoAjIuIXZA+mOxhoN7oyy5P/ujHbu3pLWl20viwiCrc/HyDp12R/BM5IsauB2yVdS/aE0M+k+F8AC9Osv81kieeVDvZZDdwl6UNkD/m7pYc+yto+wHzNxqwHSNdsaiNia6X7YpYHn0YzM7PceWRjZma588jGzMxy52RjZma5c7IxM7PcOdmYmVnunGzMzCx3/x+fX6mpoLZQNwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Model Misclassification Error (Train): 0.45592705167173253\n",
      "Final Model Misclassification Error (Validation): 0.45785876993166286\n",
      "Final Model Misclassification Error (Test): 0.39636363636363636\n",
      "Final Model weights: {'hidden_layer1_weights': array([[-0.96792761],\n",
      "       [ 0.98925352],\n",
      "       [-0.62533512],\n",
      "       [ 0.20527398]]), 'hidden_layer2_weights': array([[ 8.55516500e-01,  8.30306052e-04, -6.94698834e-02,\n",
      "         4.28529108e-01, -3.84433230e-01]]), 'output_layer_weights': array([[-0.01097551],\n",
      "       [-1.02964219],\n",
      "       [ 0.53528221],\n",
      "       [ 0.23070648],\n",
      "       [-0.08545835]])}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_data(file_path):\n",
    "    data = pd.read_csv(file_path, header=None, names=['Feature1', 'Feature2', 'Feature3', 'Feature4', 'Class'])\n",
    "    X = data.iloc[:, :-1].values\n",
    "    y = data.iloc[:, -1].values\n",
    "    return X, y\n",
    "\n",
    "def split_data(X, y, test_size, validation_size, random_state):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "    X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=validation_size, random_state=random_state)\n",
    "    return X_train, y_train, X_validation, y_validation, X_test, y_test\n",
    "    \n",
    "def standardize_data(X_train, X_validation, X_test):\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_validation = scaler.transform(X_validation)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    return X_train, X_validation, X_test\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "def initialize_weights(input_dim, n1, n2, random_state):\n",
    "    np.random.seed(random_state)\n",
    "    hidden_layer1_weights = 2 * np.random.rand(input_dim, n1) - 1\n",
    "    hidden_layer2_weights = 2 * np.random.rand(n1, n2) - 1\n",
    "    output_layer_weights = 2 * np.random.rand(n2, 1) - 1\n",
    "    return hidden_layer1_weights, hidden_layer2_weights, output_layer_weights\n",
    "   \n",
    "def forward_propagation(X, hidden_layer1_weights, hidden_layer2_weights, output_layer_weights):\n",
    "    hidden_layer1_output = sigmoid(np.dot(X, hidden_layer1_weights))\n",
    "    hidden_layer2_output = sigmoid(np.dot(hidden_layer1_output, hidden_layer2_weights))\n",
    "    output = sigmoid(np.dot(hidden_layer2_output, output_layer_weights))\n",
    "    return hidden_layer1_output, hidden_layer2_output, output\n",
    "\n",
    "def compute_loss(y, output):\n",
    "    loss = -np.mean(y * np.log(output) + (1 - y) * np.log(1 - output))\n",
    "    return loss\n",
    "\n",
    "def train_neural_network_with_loss_collection(X_train, y_train, X_validation, y_validation, hidden_layer1_weights,\n",
    "                                              hidden_layer2_weights, output_layer_weights, learning_rate, epochs,\n",
    "                                              early_stopping_patience):\n",
    "    training_loss_values = []\n",
    "    validation_loss_values = []\n",
    "    \n",
    "    best_validation_loss = float('inf')\n",
    "    patience = 0\n",
    "    best_weights = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        hidden_layer1_output, hidden_layer2_output, output = forward_propagation(X_train, hidden_layer1_weights, hidden_layer2_weights, output_layer_weights)\n",
    "\n",
    "        training_loss = compute_loss(y_train, output)\n",
    "        training_loss_values.append(training_loss)\n",
    "\n",
    "        hidden_layer1_output_val, hidden_layer2_output_val, output_val = forward_propagation(X_validation, hidden_layer1_weights, hidden_layer2_weights, output_layer_weights)\n",
    "        validation_loss = compute_loss(y_validation, output_val)\n",
    "        validation_loss_values.append(validation_loss)\n",
    "\n",
    "        if validation_loss < best_validation_loss:\n",
    "            best_weights = {\n",
    "                'hidden_layer1_weights': hidden_layer1_weights.copy(),\n",
    "                'hidden_layer2_weights': hidden_layer2_weights.copy(),\n",
    "                'output_layer_weights': output_layer_weights.copy()\n",
    "            }\n",
    "            best_validation_loss = validation_loss\n",
    "            best_training_loss = training_loss\n",
    "            patience = 0\n",
    "        else:\n",
    "            patience += 1\n",
    "            if patience >= early_stopping_patience:\n",
    "                print(f'Early stopping at epoch {epoch}')\n",
    "                break\n",
    "\n",
    "        output_error = y_train.reshape(-1, 1) - output\n",
    "        output_delta = output_error * sigmoid_derivative(output)\n",
    "\n",
    "        hidden_layer2_error = output_delta.dot(output_layer_weights.T)\n",
    "        hidden_layer2_delta = hidden_layer2_error * sigmoid_derivative(hidden_layer2_output)\n",
    "\n",
    "        hidden_layer1_error = hidden_layer2_delta.dot(hidden_layer2_weights.T)\n",
    "        hidden_layer1_delta = hidden_layer1_error * sigmoid_derivative(hidden_layer1_output)\n",
    "\n",
    "        output_layer_weights += hidden_layer2_output.T.dot(output_delta) * learning_rate\n",
    "        hidden_layer2_weights += hidden_layer1_output.T.dot(hidden_layer2_delta) * learning_rate\n",
    "        hidden_layer1_weights += X_train.T.dot(hidden_layer1_delta) * learning_rate\n",
    "\n",
    "    return best_weights, best_training_loss, best_validation_loss, training_loss_values, validation_loss_values\n",
    "\n",
    "\n",
    "def test_neural_network(X, y, best_weights):\n",
    "    hidden_layer1_output, hidden_layer2_output, output = forward_propagation(X, best_weights['hidden_layer1_weights'], best_weights['hidden_layer2_weights'], best_weights['output_layer_weights'])\n",
    "    test_predictions = (output > 0.5).astype(int)\n",
    "    misclassification_error = np.mean(test_predictions.flatten() != y)\n",
    "    return misclassification_error\n",
    "\n",
    "def plot_learning_curves(training_loss, validation_loss):\n",
    "    plt.plot(training_loss, label='Training Loss')\n",
    "    plt.plot(validation_loss, label='Validation Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    #plt.ylim(0.688, 0.69)  # Set y-axis limits\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def main():\n",
    "    file_path = 'data_banknote_authentication.txt'\n",
    "    test_size = 0.2\n",
    "    validation_size = 0.4\n",
    "    random_state = 2782  \n",
    "    learning_rate = 0.005\n",
    "    #learning_rate = 0.01\n",
    "    epochs = 100\n",
    "    early_stopping_patience = 15   #allows model to stop earlier if validation loss stops decreasing\n",
    "    max_hidden_units = 8    #set to ensure that n1 + n2 does not exceed 8\n",
    "\n",
    "    best_network = {\n",
    "        'n1': None,\n",
    "        'n2': None,\n",
    "        'best_weights': None,\n",
    "        'best_validation_loss': float('inf'),\n",
    "        'best_misclassification_error': float('inf')\n",
    "    }\n",
    "\n",
    "    for n1 in range(1, max_hidden_units):\n",
    "        for n2 in range(1, max_hidden_units - n1 + 1):\n",
    "            print(f'\\nTraining for n1={n1}, n2={n2}')\n",
    "            best_weights_list = []\n",
    "            best_training_loss_list = []\n",
    "            best_validation_loss_list = []\n",
    "            misclassification_error_test_list = []\n",
    "\n",
    "            for run in range(3):  # Run at least three times with different initial weights\n",
    "                X, y = load_data(file_path)\n",
    "                X_train, y_train, X_validation, y_validation, X_test, y_test = split_data(X, y, test_size, validation_size, random_state)\n",
    "                X_train, X_validation, X_test = standardize_data(X_train, X_validation, X_test)\n",
    "                input_dim = X_train.shape[1]\n",
    "                hidden_layer1_weights, hidden_layer2_weights, output_layer_weights = initialize_weights(input_dim, n1, n2, random_state + run)\n",
    "\n",
    "                best_weights, best_training_loss, best_validation_loss, training_loss_values, validation_loss_values = train_neural_network_with_loss_collection(X_train, y_train, X_validation, y_validation, hidden_layer1_weights, hidden_layer2_weights, output_layer_weights, learning_rate, epochs, early_stopping_patience)\n",
    "                misclassification_error_test = test_neural_network(X_test, y_test, best_weights)\n",
    "\n",
    "                best_weights_list.append(best_weights)\n",
    "                best_training_loss_list.append(best_training_loss)\n",
    "                best_validation_loss_list.append(best_validation_loss)\n",
    "                misclassification_error_test_list.append(misclassification_error_test)\n",
    "\n",
    "                print(f'Run {run + 1}: Best Training Loss: {best_training_loss}, Best Validation Loss: {best_validation_loss}, Misclassification Error (Test): {misclassification_error_test}')\n",
    "\n",
    "            # Choose the best initialization for this network configuration\n",
    "            best_index = np.argmin(best_validation_loss_list)\n",
    "            if best_validation_loss_list[best_index] < best_network['best_validation_loss']:\n",
    "                best_network['n1'] = n1\n",
    "                best_network['n2'] = n2\n",
    "                best_network['best_weights'] = best_weights_list[best_index]\n",
    "                best_network['best_training_loss'] = best_training_loss_list[best_index]\n",
    "                best_network['best_validation_loss'] = best_validation_loss_list[best_index]\n",
    "                best_network['best_misclassification_error'] = misclassification_error_test_list[best_index]\n",
    "\n",
    "    # Print and plot results for the best network\n",
    "    print(f'\\nBest Network Configuration: n1={best_network[\"n1\"]}, n2={best_network[\"n2\"]}')\n",
    "    print(f'Best Validation Loss: {best_network[\"best_validation_loss\"]}')\n",
    "    print(f'Best Training Loss: {best_network[\"best_training_loss\"]}')\n",
    "    print(f'Best Misclassification Error (Test): {best_network[\"best_misclassification_error\"]}')\n",
    "\n",
    "    # Extract loss values during training and validation from your training loop\n",
    "    best_weights = best_network['best_weights']\n",
    "    best_training_loss_values, best_validation_loss_values = train_neural_network_with_loss_collection(X_train, y_train, X_validation, y_validation, best_weights['hidden_layer1_weights'], best_weights['hidden_layer2_weights'], best_weights['output_layer_weights'], learning_rate, epochs, early_stopping_patience)[-2:]\n",
    "\n",
    "    # Plot learning curves for the best network\n",
    "    #plot_learning_curves(training_loss_values, validation_loss_values)\n",
    "    plot_learning_curves(best_training_loss_values, best_validation_loss_values)\n",
    "\n",
    "    # Evaluate misclassification error for the final model\n",
    "    final_model_misclassification_error_train = test_neural_network(X_train, y_train, best_weights)\n",
    "    final_model_misclassification_error_validation = test_neural_network(X_validation, y_validation, best_weights)\n",
    "    final_model_misclassification_error_test = test_neural_network(X_test, y_test, best_weights)\n",
    "\n",
    "    print(f'\\nFinal Model Misclassification Error (Train): {final_model_misclassification_error_train}')\n",
    "    print(f'Final Model Misclassification Error (Validation): {final_model_misclassification_error_validation}')\n",
    "    print(f'Final Model Misclassification Error (Test): {final_model_misclassification_error_test}')\n",
    "    print(f'Final Model weights: {best_weights}')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
