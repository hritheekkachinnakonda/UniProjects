{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n1 = 1, n2 = 1:\n",
      "\t Run = 1\n",
      "\t Epoch (early stopping) = 65\n",
      "\t Validation Loss = 0.6895967830354958\n",
      "\t Training Loss = 0.6892709741394527\n",
      "n1 = 1, n2 = 1:\n",
      "\t Run = 2\n",
      "\t Epoch (early stopping) = 55\n",
      "\t Validation Loss = 0.6895930280571754\n",
      "\t Training Loss = 0.6892674461015283\n",
      "n1 = 1, n2 = 1:\n",
      "\t Run = 3\n",
      "\t Epoch (early stopping) = 118\n",
      "\t Validation Loss = 0.6895917947578903\n",
      "\t Training Loss = 0.689265405651643\n",
      "n1 = 1, n2 = 2:\n",
      "\t Run = 1\n",
      "\t Epoch (early stopping) = 128\n",
      "\t Validation Loss = 0.6895913637762057\n",
      "\t Training Loss = 0.6892647542842448\n",
      "n1 = 1, n2 = 2:\n",
      "\t Run = 2\n",
      "\t Epoch (early stopping) = 89\n",
      "\t Validation Loss = 0.6895912422385956\n",
      "\t Training Loss = 0.6892641231236454\n",
      "n1 = 1, n2 = 2:\n",
      "\t Run = 3\n",
      "\t Epoch (early stopping) = 67\n",
      "\t Validation Loss = 0.689594829769509\n",
      "\t Training Loss = 0.6892720984268356\n",
      "n1 = 1, n2 = 3:\n",
      "\t Run = 1\n",
      "\t Epoch (early stopping) = 72\n",
      "\t Validation Loss = 0.6895912436335749\n",
      "\t Training Loss = 0.6892643182767586\n",
      "n1 = 1, n2 = 3:\n",
      "\t Run = 2\n",
      "\t Epoch (early stopping) = 75\n",
      "\t Validation Loss = 0.6896050315174875\n",
      "\t Training Loss = 0.6892592229304484\n",
      "n1 = 1, n2 = 3:\n",
      "\t Run = 3\n",
      "\t Epoch (early stopping) = 57\n",
      "\t Validation Loss = 0.689597098821254\n",
      "\t Training Loss = 0.6892710950897003\n",
      "n1 = 1, n2 = 4:\n",
      "\t Run = 1\n",
      "\t Epoch (early stopping) = 55\n",
      "\t Validation Loss = 0.6896092615264545\n",
      "\t Training Loss = 0.6892874253691235\n",
      "n1 = 1, n2 = 4:\n",
      "\t Run = 2\n",
      "\t Epoch (early stopping) = 53\n",
      "\t Validation Loss = 0.6895927254620666\n",
      "\t Training Loss = 0.6892657046386029\n",
      "n1 = 1, n2 = 4:\n",
      "\t Run = 3\n",
      "\t Epoch (early stopping) = 65\n",
      "\t Validation Loss = 0.6895913887126143\n",
      "\t Training Loss = 0.6892670566478208\n",
      "n1 = 1, n2 = 5:\n",
      "\t Run = 1\n",
      "\t Epoch (early stopping) = 58\n",
      "\t Validation Loss = 0.6895913421128309\n",
      "\t Training Loss = 0.6892659202782306\n",
      "n1 = 1, n2 = 5:\n",
      "\t Run = 2\n",
      "\t Epoch (early stopping) = 163\n",
      "\t Validation Loss = 0.6895965231369879\n",
      "\t Training Loss = 0.6892585564393884\n",
      "n1 = 1, n2 = 5:\n",
      "\t Run = 3\n",
      "\t Epoch (early stopping) = 68\n",
      "\t Validation Loss = 0.6895967050786193\n",
      "\t Training Loss = 0.6892581597191985\n",
      "n1 = 1, n2 = 6:\n",
      "\t Run = 1\n",
      "\t Epoch (early stopping) = 47\n",
      "\t Validation Loss = 0.689592452356096\n",
      "\t Training Loss = 0.6892676150770113\n",
      "n1 = 1, n2 = 6:\n",
      "\t Run = 2\n",
      "\t Epoch (early stopping) = 54\n",
      "\t Validation Loss = 0.689601703154386\n",
      "\t Training Loss = 0.6892713893737114\n",
      "n1 = 1, n2 = 6:\n",
      "\t Run = 3\n",
      "\t Epoch (early stopping) = 57\n",
      "\t Validation Loss = 0.6895912565562018\n",
      "\t Training Loss = 0.6892661501576898\n",
      "n1 = 1, n2 = 7:\n",
      "\t Run = 1\n",
      "\t Epoch (early stopping) = 52\n",
      "\t Validation Loss = 0.6895937816032565\n",
      "\t Training Loss = 0.6892711100613875\n",
      "n1 = 1, n2 = 7:\n",
      "\t Run = 2\n",
      "\t Epoch (early stopping) = 51\n",
      "\t Validation Loss = 0.689591620799425\n",
      "\t Training Loss = 0.6892680397323524\n",
      "n1 = 1, n2 = 7:\n",
      "\t Run = 3\n",
      "\t Epoch (early stopping) = 45\n",
      "\t Validation Loss = 0.6895914699403546\n",
      "\t Training Loss = 0.6892624489208332\n",
      "n1 = 2, n2 = 1:\n",
      "\t Run = 1\n",
      "\t Epoch (early stopping) = 68\n",
      "\t Validation Loss = 0.6895986247437218\n",
      "\t Training Loss = 0.6892707605901864\n",
      "n1 = 2, n2 = 1:\n",
      "\t Run = 2\n",
      "\t Epoch (early stopping) = 90\n",
      "\t Validation Loss = 0.6897966587084701\n",
      "\t Training Loss = 0.6893931184845323\n",
      "n1 = 2, n2 = 1:\n",
      "\t Run = 3\n",
      "\t Epoch (early stopping) = 52\n",
      "\t Validation Loss = 0.6895945214649206\n",
      "\t Training Loss = 0.6892692366935605\n",
      "n1 = 2, n2 = 2:\n",
      "\t Run = 1\n",
      "\t Epoch (early stopping) = 87\n",
      "\t Validation Loss = 0.6895962512890135\n",
      "\t Training Loss = 0.6892710358076467\n",
      "n1 = 2, n2 = 2:\n",
      "\t Run = 2\n",
      "\t Epoch (early stopping) = 110\n",
      "\t Validation Loss = 0.6895912342086119\n",
      "\t Training Loss = 0.6892646987341833\n",
      "n1 = 2, n2 = 2:\n",
      "\t Run = 3\n",
      "\t Epoch (early stopping) = 79\n",
      "\t Validation Loss = 0.6895992772199815\n",
      "\t Training Loss = 0.6892761415409162\n",
      "n1 = 2, n2 = 3:\n",
      "\t Run = 1\n",
      "\t Epoch (early stopping) = 76\n",
      "\t Validation Loss = 0.6896680553012375\n",
      "\t Training Loss = 0.6892941093680159\n",
      "n1 = 2, n2 = 3:\n",
      "\t Run = 2\n",
      "\t Epoch (early stopping) = 66\n",
      "\t Validation Loss = 0.6895914180741508\n",
      "\t Training Loss = 0.6892642332428043\n",
      "n1 = 2, n2 = 3:\n",
      "\t Run = 3\n",
      "\t Epoch (early stopping) = 66\n",
      "\t Validation Loss = 0.6895916913649934\n",
      "\t Training Loss = 0.6892646619746629\n",
      "n1 = 2, n2 = 4:\n",
      "\t Run = 1\n",
      "\t Epoch (early stopping) = 317\n",
      "\t Validation Loss = 0.6895971986460856\n",
      "\t Training Loss = 0.689258334140284\n",
      "n1 = 2, n2 = 4:\n",
      "\t Run = 2\n",
      "\t Epoch (early stopping) = 58\n",
      "\t Validation Loss = 0.6895921489105223\n",
      "\t Training Loss = 0.6892611587516965\n",
      "n1 = 2, n2 = 4:\n",
      "\t Run = 3\n",
      "\t Epoch (early stopping) = 302\n",
      "\t Validation Loss = 0.6895951519323668\n",
      "\t Training Loss = 0.6892595498049575\n",
      "n1 = 2, n2 = 5:\n",
      "\t Run = 1\n",
      "\t Epoch (early stopping) = 78\n",
      "\t Validation Loss = 0.6895934500520501\n",
      "\t Training Loss = 0.6892747165607096\n",
      "n1 = 2, n2 = 5:\n",
      "\t Run = 2\n",
      "\t Epoch (early stopping) = 59\n",
      "\t Validation Loss = 0.6895914555159813\n",
      "\t Training Loss = 0.689265183331066\n",
      "n1 = 2, n2 = 5:\n",
      "\t Run = 3\n",
      "\t Epoch (early stopping) = 62\n",
      "\t Validation Loss = 0.6895944548709413\n",
      "\t Training Loss = 0.6892703546994643\n",
      "n1 = 2, n2 = 6:\n",
      "\t Run = 1\n",
      "\t Epoch (early stopping) = 45\n",
      "\t Validation Loss = 0.6895943790857043\n",
      "\t Training Loss = 0.6892631760635665\n",
      "n1 = 2, n2 = 6:\n",
      "\t Run = 2\n",
      "\t Epoch (early stopping) = 56\n",
      "\t Validation Loss = 0.6895917158298418\n",
      "\t Training Loss = 0.6892632466788483\n",
      "n1 = 2, n2 = 6:\n",
      "\t Run = 3\n",
      "\t Epoch (early stopping) = 58\n",
      "\t Validation Loss = 0.6895930597167947\n",
      "\t Training Loss = 0.689265628532748\n",
      "n1 = 3, n2 = 1:\n",
      "\t Run = 1\n",
      "\t Epoch (early stopping) = 74\n",
      "\t Validation Loss = 0.6895934333702147\n",
      "\t Training Loss = 0.6892672713307691\n",
      "n1 = 3, n2 = 1:\n",
      "\t Run = 2\n",
      "\t Epoch (early stopping) = 103\n",
      "\t Validation Loss = 0.6895926046560839\n",
      "\t Training Loss = 0.6892665518429021\n",
      "n1 = 3, n2 = 1:\n",
      "\t Run = 3\n",
      "\t Epoch (early stopping) = 80\n",
      "\t Validation Loss = 0.6895934498509023\n",
      "\t Training Loss = 0.6892684873609157\n",
      "n1 = 3, n2 = 2:\n",
      "\t Run = 1\n",
      "\t Epoch (early stopping) = 203\n",
      "\t Validation Loss = 0.6895924918229192\n",
      "\t Training Loss = 0.6892664963617806\n",
      "n1 = 3, n2 = 2:\n",
      "\t Run = 2\n",
      "\t Epoch (early stopping) = 64\n",
      "\t Validation Loss = 0.6895925755482507\n",
      "\t Training Loss = 0.68926355051306\n",
      "n1 = 3, n2 = 2:\n",
      "\t Run = 3\n",
      "\t Epoch (early stopping) = 85\n",
      "\t Validation Loss = 0.6895939089701133\n",
      "\t Training Loss = 0.6892682128244277\n",
      "n1 = 3, n2 = 3:\n",
      "\t Run = 1\n",
      "\t Epoch (early stopping) = 61\n",
      "\t Validation Loss = 0.6895918275951494\n",
      "\t Training Loss = 0.6892647668548577\n",
      "n1 = 3, n2 = 3:\n",
      "\t Run = 2\n",
      "\t Epoch (early stopping) = 66\n",
      "\t Validation Loss = 0.6896096637855567\n",
      "\t Training Loss = 0.6892901794828675\n",
      "n1 = 3, n2 = 3:\n",
      "\t Run = 3\n",
      "\t Epoch (early stopping) = 74\n",
      "\t Validation Loss = 0.6895913210555231\n",
      "\t Training Loss = 0.689264382561486\n",
      "n1 = 3, n2 = 4:\n",
      "\t Run = 1\n",
      "\t Epoch (early stopping) = 72\n",
      "\t Validation Loss = 0.6896125605702875\n",
      "\t Training Loss = 0.6892609750233789\n",
      "n1 = 3, n2 = 4:\n",
      "\t Run = 2\n",
      "\t Epoch (early stopping) = 61\n",
      "\t Validation Loss = 0.6895918406794281\n",
      "\t Training Loss = 0.6892673201175905\n",
      "n1 = 3, n2 = 4:\n",
      "\t Run = 3\n",
      "\t Epoch (early stopping) = 50\n",
      "\t Validation Loss = 0.6895923305326193\n",
      "\t Training Loss = 0.6892647144051453\n",
      "n1 = 3, n2 = 5:\n",
      "\t Run = 1\n",
      "\t Epoch (early stopping) = 58\n",
      "\t Validation Loss = 0.6895930351394162\n",
      "\t Training Loss = 0.6892639734906763\n",
      "n1 = 3, n2 = 5:\n",
      "\t Run = 2\n",
      "\t Epoch (early stopping) = 62\n",
      "\t Validation Loss = 0.689614174417179\n",
      "\t Training Loss = 0.6892624274960649\n",
      "n1 = 3, n2 = 5:\n",
      "\t Run = 3\n",
      "\t Epoch (early stopping) = 63\n",
      "\t Validation Loss = 0.6895918802993445\n",
      "\t Training Loss = 0.6892678918627524\n",
      "n1 = 4, n2 = 1:\n",
      "\t Run = 1\n",
      "\t Epoch (early stopping) = 156\n",
      "\t Validation Loss = 0.6895914644014426\n",
      "\t Training Loss = 0.6892658954581752\n",
      "n1 = 4, n2 = 1:\n",
      "\t Run = 2\n",
      "\t Epoch (early stopping) = 121\n",
      "\t Validation Loss = 0.6895958075339278\n",
      "\t Training Loss = 0.6892694353577724\n",
      "n1 = 4, n2 = 1:\n",
      "\t Run = 3\n",
      "\t Epoch (early stopping) = 152\n",
      "\t Validation Loss = 0.689592900035373\n",
      "\t Training Loss = 0.6892676063793933\n",
      "n1 = 4, n2 = 2:\n",
      "\t Run = 1\n",
      "\t Epoch (early stopping) = 77\n",
      "\t Validation Loss = 0.689593926357376\n",
      "\t Training Loss = 0.6892676549503102\n",
      "n1 = 4, n2 = 2:\n",
      "\t Run = 2\n",
      "\t Epoch (early stopping) = 90\n",
      "\t Validation Loss = 0.6896151945404326\n",
      "\t Training Loss = 0.6893109575889679\n",
      "n1 = 4, n2 = 2:\n",
      "\t Run = 3\n",
      "\t Epoch (early stopping) = 68\n",
      "\t Validation Loss = 0.6895930637871645\n",
      "\t Training Loss = 0.6892677042628992\n",
      "n1 = 4, n2 = 3:\n",
      "\t Run = 1\n",
      "\t Epoch (early stopping) = 59\n",
      "\t Validation Loss = 0.6896200268124576\n",
      "\t Training Loss = 0.6892650223239997\n",
      "n1 = 4, n2 = 3:\n",
      "\t Run = 2\n",
      "\t Epoch (early stopping) = 56\n",
      "\t Validation Loss = 0.689592960073055\n",
      "\t Training Loss = 0.6892683522345548\n",
      "n1 = 4, n2 = 3:\n",
      "\t Run = 3\n",
      "\t Epoch (early stopping) = 56\n",
      "\t Validation Loss = 0.6895925670161897\n",
      "\t Training Loss = 0.6892654888561489\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n1 = 4, n2 = 4:\n",
      "\t Run = 1\n",
      "\t Epoch (early stopping) = 56\n",
      "\t Validation Loss = 0.6895926671261201\n",
      "\t Training Loss = 0.6892631858795379\n",
      "n1 = 4, n2 = 4:\n",
      "\t Run = 2\n",
      "\t Epoch (early stopping) = 69\n",
      "\t Validation Loss = 0.6895919005009412\n",
      "\t Training Loss = 0.6892652893460033\n",
      "n1 = 4, n2 = 4:\n",
      "\t Run = 3\n",
      "\t Epoch (early stopping) = 62\n",
      "\t Validation Loss = 0.6895914574520704\n",
      "\t Training Loss = 0.6892648003703334\n",
      "n1 = 5, n2 = 1:\n",
      "\t Run = 1\n",
      "\t Epoch (early stopping) = 58\n",
      "\t Validation Loss = 0.6895915310061194\n",
      "\t Training Loss = 0.6892645419218923\n",
      "n1 = 5, n2 = 1:\n",
      "\t Run = 2\n",
      "\t Epoch (early stopping) = 85\n",
      "\t Validation Loss = 0.6895960768063133\n",
      "\t Training Loss = 0.6892702958801704\n",
      "n1 = 5, n2 = 1:\n",
      "\t Run = 3\n",
      "\t Epoch (early stopping) = 75\n",
      "\t Validation Loss = 0.6895946035764786\n",
      "\t Training Loss = 0.6892697266165555\n",
      "n1 = 5, n2 = 2:\n",
      "\t Run = 1\n",
      "\t Epoch (early stopping) = 70\n",
      "\t Validation Loss = 0.6895920763620983\n",
      "\t Training Loss = 0.6892669575214866\n",
      "n1 = 5, n2 = 2:\n",
      "\t Run = 2\n",
      "\t Epoch (early stopping) = 76\n",
      "\t Validation Loss = 0.6895918797375317\n",
      "\t Training Loss = 0.6892660172849072\n",
      "n1 = 5, n2 = 2:\n",
      "\t Run = 3\n",
      "\t Epoch (early stopping) = 69\n",
      "\t Validation Loss = 0.6895929091930959\n",
      "\t Training Loss = 0.6892643585571437\n",
      "n1 = 5, n2 = 3:\n",
      "\t Run = 1\n",
      "\t Epoch (early stopping) = 58\n",
      "\t Validation Loss = 0.6895941534128646\n",
      "\t Training Loss = 0.689271799549912\n",
      "n1 = 5, n2 = 3:\n",
      "\t Run = 2\n",
      "\t Epoch (early stopping) = 65\n",
      "\t Validation Loss = 0.6895931431323118\n",
      "\t Training Loss = 0.6892670722757298\n",
      "n1 = 5, n2 = 3:\n",
      "\t Run = 3\n",
      "\t Epoch (early stopping) = 56\n",
      "\t Validation Loss = 0.6895993568665074\n",
      "\t Training Loss = 0.6892759590408672\n",
      "n1 = 6, n2 = 1:\n",
      "\t Run = 1\n",
      "\t Epoch (early stopping) = 178\n",
      "\t Validation Loss = 0.6895922407894861\n",
      "\t Training Loss = 0.6892657624052196\n",
      "n1 = 6, n2 = 1:\n",
      "\t Run = 2\n",
      "\t Epoch (early stopping) = 130\n",
      "\t Validation Loss = 0.6895956512221031\n",
      "\t Training Loss = 0.6892698982553617\n",
      "n1 = 6, n2 = 1:\n",
      "\t Run = 3\n",
      "\t Epoch (early stopping) = 90\n",
      "\t Validation Loss = 0.6895923387290545\n",
      "\t Training Loss = 0.689265809687022\n",
      "n1 = 6, n2 = 2:\n",
      "\t Run = 1\n",
      "\t Epoch (early stopping) = 117\n",
      "\t Validation Loss = 0.6895920584373773\n",
      "\t Training Loss = 0.6892651017936566\n",
      "n1 = 6, n2 = 2:\n",
      "\t Run = 2\n",
      "\t Epoch (early stopping) = 65\n",
      "\t Validation Loss = 0.6895915944624478\n",
      "\t Training Loss = 0.6892660352957094\n",
      "n1 = 6, n2 = 2:\n",
      "\t Run = 3\n",
      "\t Epoch (early stopping) = 76\n",
      "\t Validation Loss = 0.6895948746570736\n",
      "\t Training Loss = 0.6892691911558069\n",
      "n1 = 7, n2 = 1:\n",
      "\t Run = 1\n",
      "\t Epoch (early stopping) = 66\n",
      "\t Validation Loss = 0.6895982765358449\n",
      "\t Training Loss = 0.6892723728576201\n",
      "n1 = 7, n2 = 1:\n",
      "\t Run = 2\n",
      "\t Epoch (early stopping) = 67\n",
      "\t Validation Loss = 0.6895959733598118\n",
      "\t Training Loss = 0.6892716372158798\n",
      "n1 = 7, n2 = 1:\n",
      "\t Run = 3\n",
      "\t Epoch (early stopping) = 96\n",
      "\t Validation Loss = 0.6895916127623781\n",
      "\t Training Loss = 0.689265754705322\n",
      "\n",
      "THE FINAL MODEL (ERRORS and WEIGHTS):n1=2, n2=2\n",
      "\n",
      "\tTraining Loss: 0.689265754705322\n",
      "\tValidation Loss: 0.6895912342086119\n",
      "\n",
      "\tTest Misclassification Error: 0.39636363636363636\n",
      "\tTraining Misclassification Error: 0.45592705167173253\n",
      "\tValidation Misclassification Error: 0.45785876993166286\n",
      "\n",
      "Hidden Layer 1 Weights: [[-0.05668723 -0.07832953]\n",
      " [-0.15455762  0.19406176]\n",
      " [-0.1536965  -0.15776204]\n",
      " [ 0.04848469  0.36638581]]\n",
      "\n",
      "Hidden Layer 2 Weights: [[ 0.39144226 -0.11701077]\n",
      " [-0.41561928 -0.04674589]]\n",
      "\n",
      "Output Layer Weights: [[-0.01214964]\n",
      " [-0.33991867]]\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Hritheekka Chinnakonda, chinnakh, 400292782\n",
    "\n",
    "# importing libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Neural Network Classifier class\n",
    "class Neural_Network_Classifier:\n",
    "    # initializing all necessary values and variables\n",
    "    def __init__(self):\n",
    "        self.hidden_L1_weights = None\n",
    "        self.hidden_L2_weights = None\n",
    "        self.output_layer_weights = None\n",
    "        self.learning_rate = 0.005\n",
    "        self.epochs = 1000\n",
    "        self.early_stopping_patience = 45\n",
    "        self.max_hidden_units = 8 # maximum of 8 hidden layers\n",
    "        \n",
    "    # initializing the weights for hidden layer 1, hidden layer 2 and the output layer\n",
    "    def weights(self, num_input_units, n1, n2):\n",
    "        \n",
    "        # initializing limits for weights, based on the number of input units\n",
    "        hidden_limit = np.sqrt(1 / (num_input_units + n1)) # limits for the hidden layers\n",
    "        outer_limit = np.sqrt(1 / (n1 + n2)) # limits between 2 hidden layers and output layer\n",
    "        \n",
    "        # generate random weights using a uniform distribution within limits set above \n",
    "        self.hidden_L1_weights = np.random.uniform(-hidden_limit, hidden_limit, size=(num_input_units, n1))\n",
    "        self.hidden_L2_weights = np.random.uniform(-outer_limit, outer_limit, size=(n1, n2))\n",
    "        self.output_layer_weights = np.random.uniform(-outer_limit, outer_limit, size=(n2, 1))\n",
    "    \n",
    "    \n",
    "    # logistic sigmoid activation function for output and hidden layers\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    # derivative of logistic sigmoid activation function for backpropagation \n",
    "    def sigmoid_derivative(self, z):\n",
    "        return self.sigmoid(z) * (1 - self.sigmoid(z))\n",
    "    \n",
    "     # computing cross entropy loss \n",
    "    def cross_entropy(self, y_true, output):\n",
    "        cross_entropy_loss = -np.mean(y_true * np.log(output) + (1 - y_true) * np.log(1 - output))\n",
    "        return cross_entropy_loss\n",
    "\n",
    "    # forward propagation     \n",
    "    def forward_propagation(self, X):\n",
    "        \n",
    "        # computing the outputs using the sigmoid activation function and weights\n",
    "        hidden_L1_output = self.sigmoid(np.dot(X, self.hidden_L1_weights))\n",
    "        hidden_L2_output = self.sigmoid(np.dot(hidden_L1_output, self.hidden_L2_weights))\n",
    "        output_layer_out = self.sigmoid(np.dot(hidden_L2_output, self.output_layer_weights))\n",
    "        return hidden_L1_output, hidden_L2_output, output_layer_out\n",
    "\n",
    "\n",
    "    # backpropagation \n",
    "    def backpropagation(self, X_train, y_train):\n",
    "        \n",
    "        # receiving output of each layer from forward propagation function\n",
    "        hidden_L1_output, hidden_L2_output, output_layer_out = self.forward_propagation(X_train)\n",
    "\n",
    "        # calculating error at ouput layer, error-weighted derivative using activation function\n",
    "        output_error = y_train.reshape(-1, 1) - output_layer_out\n",
    "        output_delta = output_error * self.sigmoid_derivative(output_layer_out)\n",
    "\n",
    "        # error is backpropagated from the output layer to the hidden layers\n",
    "        # calculating error at hidden layer 2, error-weighted derivative using activation function\n",
    "        hidden_L2_error = output_delta.dot(self.output_layer_weights.T)\n",
    "        hidden_L2_delta = hidden_L2_error * self.sigmoid_derivative(hidden_L2_output)\n",
    "\n",
    "        # calculating error at hidden layer 1, error-weighted derivative using activation function\n",
    "        hidden_L1_error = hidden_L2_delta.dot(self.hidden_L2_weights.T)\n",
    "        hidden_L1_delta = hidden_L1_error * self.sigmoid_derivative(hidden_L1_output)\n",
    "        \n",
    "        # updating the weights of each layer, using gradient descent \n",
    "        self.output_layer_weights = self.output_layer_weights + (hidden_L2_output.T.dot(output_delta) * self.learning_rate)\n",
    "        self.hidden_L2_weights = self.hidden_L2_weights + (hidden_L1_output.T.dot(hidden_L2_delta) * self.learning_rate)\n",
    "        self.hidden_L1_weights = self.hidden_L1_weights + (X_train.T.dot(hidden_L1_delta) * self.learning_rate)\n",
    "    \n",
    "    def train_classifier(self, X_train, y_train, X_validation, y_validation):\n",
    "        \n",
    "        # initializing necessary values and lists\n",
    "        training_loss_values = []\n",
    "        validation_loss_values = []\n",
    "        best_validation_loss = float('inf')\n",
    "        patience = 0 # tracking the number of epochs with no improvements in validation loss\n",
    "\n",
    "        # iterating through all epochs\n",
    "        for epoch in range(self.epochs):\n",
    "            \n",
    "            #performs back propagation to update weights based on training data\n",
    "            self.backpropagation(X_train, y_train)\n",
    "            \n",
    "            #performs forward propagation to get outputs of each layer\n",
    "            hidden_L1_output_val, hidden_L2_output_val, output_val = self.forward_propagation(X_validation)\n",
    "\n",
    "            # calculating training loss and appending it to list\n",
    "            training_loss = self.cross_entropy(y_train, self.forward_propagation(X_train)[-1])\n",
    "            training_loss_values.append(training_loss)\n",
    "\n",
    "            # calculating validation loss and appending it to list\n",
    "            validation_loss = self.cross_entropy(y_validation, output_val)\n",
    "            validation_loss_values.append(validation_loss)\n",
    "\n",
    "            # early stopping, comparing current validation loss with the best stored validation loss\n",
    "            if best_validation_loss > validation_loss:\n",
    "                \n",
    "                patience = 0 # reset to 0\n",
    "                \n",
    "                # updates validation and training loss\n",
    "                best_validation_loss = validation_loss\n",
    "                best_training_loss = training_loss\n",
    "                \n",
    "                # updates  weights\n",
    "                hidden_L1_weights =  self.hidden_L1_weights.copy()\n",
    "                hidden_L2_weights =  self.hidden_L2_weights.copy()\n",
    "                output_layer_weights = self.output_layer_weights.copy()\n",
    "                        \n",
    "            else:\n",
    "                patience += 1 # increment counter when validation loss doesnt improve \n",
    "                \n",
    "                # break loop if condition is met \n",
    "                if self.early_stopping_patience <= patience :\n",
    "                    break\n",
    "\n",
    "        return epoch, hidden_L1_weights, hidden_L2_weights, output_layer_weights, best_training_loss, best_validation_loss, training_loss_values, validation_loss_values\n",
    "\n",
    "    # evaluating network performance,calculates misclassification error \n",
    "    def test_neural_network(self, X, y,  hidden_L1_weights, hidden_L2_weights, output_layer_weights):\n",
    "        # layer outputs from forward propagation error\n",
    "        hidden_L1_output, hidden_L2_output, output = self.forward_propagation(X)\n",
    "        # assigning classes (greater than 0.5 are considered as class 1, less than or equal to 0.5, class 0)\n",
    "        test_predictions = (output > 0.5).astype(int) # storing class prediction\n",
    "        misclassification_error = np.mean(test_predictions.flatten() != y) # calculating mean error, boolean array (predictions match/not matching true labels)\n",
    "        return misclassification_error\n",
    "    \n",
    "def main():\n",
    "    \n",
    "    #calling Neural Network Classifier\n",
    "    neural_network = Neural_Network_Classifier()\n",
    "\n",
    "    # Loading the dataset\n",
    "    data = np.loadtxt('data_banknote_authentication.txt', delimiter=',')\n",
    "    X = data[:, :-1] # features\n",
    "    y = data[:, -1] # labels\n",
    "    \n",
    "    # initializing and assigning values\n",
    "    optimal_n1 = None\n",
    "    optimal_n2 = None\n",
    "    optimal_weight_L1 = None\n",
    "    optimal_weight_L2 = None\n",
    "    optimal_weight_L3 = None\n",
    "    optimal_validation_loss = 10000000 # large number\n",
    "    optimal_misclassification_error = 10000000 # large number\n",
    "    random_seed = 2782 # last 4 digits of student number, random seed\n",
    "    np.random.seed(random_seed)\n",
    "    iterations = 3 # at least 3 iterations starting with different initial weights\n",
    "    \n",
    "    # splitting training, test and validation set (60%, 20%, 20% respectively)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=random_seed)\n",
    "    X_train, X_validation, y_train, y_validation = train_test_split(X_train, y_train, test_size=0.4, random_state=random_seed)\n",
    "\n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_validation = scaler.transform(X_validation)\n",
    "    X_test = scaler.transform(X_test)\n",
    "\n",
    "    for n1 in range(1, neural_network.max_hidden_units):\n",
    "        for n2 in range(1, neural_network.max_hidden_units - n1 + 1):\n",
    "            # Resetting loss value lists\n",
    "            training_loss_values = []\n",
    "            validation_loss_values = []\n",
    "\n",
    "            optimal_weights_list_L1 = []\n",
    "            optimal_weights_list_L2 = []\n",
    "            optimal_weights_list_L3 = []\n",
    "            best_training_loss_list = []\n",
    "            best_validation_loss_list = []\n",
    "            misclassification_error_test_list = []\n",
    "\n",
    "            for run in range(iterations):\n",
    "                neural_network.weights(X_train.shape[1], n1, n2)\n",
    "                epoch, hidden_L1_weights, hidden_L2_weights, output_layer_weights, best_training_loss, best_validation_loss, training_loss_values, validation_loss_values = neural_network.train_classifier(X_train, y_train, X_validation, y_validation)\n",
    "                misclassification_error_test = neural_network.test_neural_network(X_test, y_test,  hidden_L1_weights, hidden_L2_weights, output_layer_weights)\n",
    "\n",
    "                optimal_weights_list_L1.append(hidden_L1_weights)\n",
    "                optimal_weights_list_L2.append(hidden_L2_weights)\n",
    "                optimal_weights_list_L3.append(output_layer_weights)\n",
    "                best_training_loss_list.append(best_training_loss)\n",
    "                best_validation_loss_list.append(best_validation_loss)\n",
    "                misclassification_error_test_list.append(misclassification_error_test)\n",
    "\n",
    "                print(f'n1 = {n1}, n2 = {n2}:\\n\\t Run = {run + 1}\\n\\t Epoch (early stopping) = {epoch}\\n\\t Validation Loss = {best_validation_loss}\\n\\t Training Loss = {best_training_loss}')\n",
    "\n",
    "                best_index = np.argmin(best_validation_loss_list)\n",
    "                if best_validation_loss_list[best_index] < optimal_validation_loss:\n",
    "                    optimal_n1 = n1\n",
    "                    optimal_n2 = n2\n",
    "                    optimal_weight_L1 = optimal_weights_list_L1[best_index] \n",
    "                    optimal_weight_L2 = optimal_weights_list_L2[best_index] \n",
    "                    optimal_weight_L3 = optimal_weights_list_L3[best_index] \n",
    "                    optimal_validation_loss = best_validation_loss_list[best_index]\n",
    "                    optimal_misclassification_error = misclassification_error_test_list[best_index]\n",
    "    \n",
    "                \n",
    "    # getting the final best model errors, using the optimal values\n",
    "    final_model_misclassification_error_test = neural_network.test_neural_network(X_test, y_test,  hidden_L1_weights, hidden_L2_weights, output_layer_weights)\n",
    "    final_model_misclassification_error_train = neural_network.test_neural_network(X_train, y_train,  hidden_L1_weights, hidden_L2_weights, output_layer_weights)\n",
    "    final_model_misclassification_error_validation = neural_network.test_neural_network(X_validation, y_validation,  hidden_L1_weights, hidden_L2_weights, output_layer_weights)\n",
    "    \n",
    "    # final model errors and weights\n",
    "    print(f'\\nTHE FINAL MODEL (ERRORS and WEIGHTS):n1={optimal_n1}, n2={optimal_n2}\\n')\n",
    "    print(f'\\tTraining Loss: {best_training_loss_list[best_index]}')\n",
    "    print(f'\\tValidation Loss: {optimal_validation_loss}\\n')\n",
    "    print(f'\\tTest Misclassification Error: {final_model_misclassification_error_test}')\n",
    "    print(f'\\tTraining Misclassification Error: {final_model_misclassification_error_train}')\n",
    "    print(f'\\tValidation Misclassification Error: {final_model_misclassification_error_validation}\\n')\n",
    "    print(f'Hidden Layer 1 Weights: {optimal_weight_L1}\\n')\n",
    "    print(f'Hidden Layer 2 Weights: {optimal_weight_L2}\\n')\n",
    "    print(f'Output Layer Weights: {optimal_weight_L3}\\n')\n",
    "    \n",
    "    \n",
    "    # plotting learning curve\n",
    "    plt.plot(training_loss_values, label='Training Loss', color='purple')\n",
    "    plt.plot(validation_loss_values, label='Validation Loss', color='blue')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
